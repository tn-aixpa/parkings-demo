{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "### 1.1. Download data\n",
    "Download data from the API, and load it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = False\n",
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/exports/csv?lang=it&timezone=UTC&use_labels=true&delimiter=%3B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execute:\n",
    "    df = pd.read_csv(URL, sep=\";\")\n",
    "    df[['lat', 'lon']] = df['coordinate'].str.split(', ',expand=True)\n",
    "    df = df.drop(columns=['% occupazione', 'GUID', 'coordinate']).rename(columns={'Parcheggio': 'parcheggio', 'Data': 'data', 'Posti liberi': 'posti_liberi', 'Posti occupati': 'posti_occupati', 'Posti totali': 'posti_totali'})\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Extract parkings\n",
    "Extract distinct parkings from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execute:\n",
    "    KEYS = ['parcheggio', 'lat', 'lon']\n",
    "    df_parcheggi = df.groupby(['parcheggio']).first().reset_index()[KEYS]\n",
    "    df_parcheggi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Aggregate Parking Data\n",
    "Aggregate Parking Data by date, hour, dow, and parking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execute: \n",
    "    rdf = df.copy()\n",
    "    rdf['data'] = pd.to_datetime(rdf['data'])\n",
    "    rdf['day'] = rdf['data'].apply(lambda t: t.replace(second=0, minute=0))\n",
    "    rdf['lat'] = rdf['lat'].apply(lambda t: float(t))\n",
    "    rdf['lon'] = rdf['lon'].apply(lambda t: float(t))\n",
    "    rdf = rdf.drop(columns=['data'])\n",
    "    grouped =rdf.groupby(['parcheggio','day']).mean()\n",
    "    df_aggregated = grouped.reset_index()\n",
    "    df_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Platform Support - Data Ops\n",
    "\n",
    "We use the platform support to load the data into the platform, version it, and automate the execution of the data management operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initalization\n",
    "Create the working context: data management project for the parking data processing. Project is a placeholder for the code, data, and management of the parking data operations. To keep it reproducible, we use the `git` source type to store the definition and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "PROJECT_NAME = \"parcheggi\"\n",
    "proj = dh.get_or_create_project(PROJECT_NAME) # source=\"git://github.com/scc-digitalhub/gdb-project-parkings.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data management functions\n",
    "We convert the data management ETL operations into functions - single executable operations that can be executed in the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/download_all_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/download_all_dh_core.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"dataset\"])\n",
    "def downloader(project, url):\n",
    "    df = url.as_df(file_format='csv',sep=\";\")\n",
    "    df[['lat', 'lon']] = df['coordinate'].str.split(', ',expand=True)\n",
    "    df = df.drop(columns=['% occupazione', 'GUID', 'coordinate']).rename(columns={'Parcheggio': 'parcheggio', 'Data': 'data', 'Posti liberi': 'posti_liberi', 'Posti occupati': 'posti_occupati', 'Posti totali': 'posti_totali'})\n",
    "    df[\"lat\"] = pd.to_numeric(df[\"lat\"])\n",
    "    df[\"lon\"] = pd.to_numeric(df[\"lon\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"downloader-funct\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/download_all_dh_core.py\", \"handler\": \"downloader\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 10:35:17,597 - INFO - Validating task.\n",
      "2024-07-04 10:35:17,598 - INFO - Validating run.\n",
      "2024-07-04 10:35:17,598 - INFO - Starting task.\n",
      "2024-07-04 10:35:17,599 - INFO - Configuring execution.\n",
      "2024-07-04 10:35:17,601 - INFO - Composing function arguments.\n",
      "2024-07-04 10:35:17,650 - INFO - Executing run.\n",
      "2024-07-04 10:35:22,898 - INFO - Task completed, returning run status.\n"
     ]
    }
   ],
   "source": [
    "run_download = func.run(action=\"job\",local_execution=True,inputs={\"url\":di.key},outputs={\"dataset\":\"dataset\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'COMPLETED', 'outputs': {'dataset': 'store://parcheggi/dataitems/table/dataset:2f2e99e6-89cd-42df-9c6c-8e67a5fddc01'}, 'results': {}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_download.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project': 'parcheggi', 'id': '6ff4a5f8-bd0f-4a8b-870a-91e2017c8259', 'kind': 'python+run', 'key': 'store://parcheggi/runs/python+run/6ff4a5f8-bd0f-4a8b-870a-91e2017c8259', 'metadata': {'project': 'parcheggi', 'name': '6ff4a5f8-bd0f-4a8b-870a-91e2017c8259', 'created': '2024-07-04T10:35:17.521Z', 'updated': '2024-07-04T10:35:22.937Z', 'created_by': 'tenant1userid', 'updated_by': 'tenant1userid'}, 'spec': {'task': 'python+job://parcheggi/downloader-funct:f0652460-b161-4276-a4e8-94c6ffd9eb4f', 'local_execution': True, 'source': {'source': 'src/download_all_dh_core.py', 'handler': 'downloader', 'base64': 'ZnJvbSBkaWdpdGFsaHViX3J1bnRpbWVfcHl0aG9uIGltcG9ydCBoYW5kbGVyCmltcG9ydCBwYW5kYXMgYXMgcGQKCkBoYW5kbGVyKG91dHB1dHM9WyJkYXRhc2V0Il0pCmRlZiBkb3dubG9hZGVyKHByb2plY3QsIHVybCk6CiAgICBkZiA9IHVybC5hc19kZihmaWxlX2Zvcm1hdD0nY3N2JyxzZXA9IjsiKQogICAgZGZbWydsYXQnLCAnbG9uJ11dID0gZGZbJ2Nvb3JkaW5hdGUnXS5zdHIuc3BsaXQoJywgJyxleHBhbmQ9VHJ1ZSkKICAgIGRmID0gZGYuZHJvcChjb2x1bW5zPVsnJSBvY2N1cGF6aW9uZScsICdHVUlEJywgJ2Nvb3JkaW5hdGUnXSkucmVuYW1lKGNvbHVtbnM9eydQYXJjaGVnZ2lvJzogJ3BhcmNoZWdnaW8nLCAnRGF0YSc6ICdkYXRhJywgJ1Bvc3RpIGxpYmVyaSc6ICdwb3N0aV9saWJlcmknLCAnUG9zdGkgb2NjdXBhdGknOiAncG9zdGlfb2NjdXBhdGknLCAnUG9zdGkgdG90YWxpJzogJ3Bvc3RpX3RvdGFsaSd9KQogICAgZGZbImxhdCJdID0gcGQudG9fbnVtZXJpYyhkZlsibGF0Il0pCiAgICBkZlsibG9uIl0gPSBwZC50b19udW1lcmljKGRmWyJsb24iXSkKICAgIHJldHVybiBkZgo=', 'lang': 'python'}, 'python_version': 'PYTHON3_9', 'function': 'python://parcheggi/downloader-funct:f0652460-b161-4276-a4e8-94c6ffd9eb4f', 'inputs': {'url': 'store://parcheggi/dataitems/table/url_data_item:6279ac1c-33aa-4a2a-8381-44ea7e4f69e8'}, 'outputs': {'dataset': 'dataset'}, 'parameters': {}}, 'status': {'state': 'COMPLETED', 'outputs': {'dataset': 'store://parcheggi/dataitems/table/dataset:2f2e99e6-89cd-42df-9c6c-8e67a5fddc01'}, 'results': {}}, 'user': 'tenant1userid'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_download.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download = run_download.outputs()['dataset'].key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_item_download = proj.new_dataitem(name=\"dataset\", kind=\"table\", path=\"s3://datalake/parcheggi/dataitems/f2024e9f-6dda-4a77-9216-80713b881300/data.parquet\")#run_download.outputs()['dataset'].key\n",
    "#data_item_download = data_item_download.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/extract_parkings_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/extract_parkings_dh_core.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"parkings\"])\n",
    "def extract_parkings(project, di):\n",
    "    KEYS = ['parcheggio', 'lat', 'lon', 'posti_totali']\n",
    "    df_parcheggi = di.as_df().groupby(['parcheggio']).first().reset_index()[KEYS]\n",
    "    return df_parcheggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"extract-parkings\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/extract_parkings_dh_core.py\", \"handler\": \"extract_parkings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 10:35:23,288 - INFO - Validating task.\n",
      "2024-07-04 10:35:23,289 - INFO - Validating run.\n",
      "2024-07-04 10:35:23,289 - INFO - Starting task.\n",
      "2024-07-04 10:35:23,290 - INFO - Configuring execution.\n",
      "2024-07-04 10:35:23,291 - INFO - Composing function arguments.\n",
      "2024-07-04 10:35:23,351 - INFO - Executing run.\n",
      "2024-07-04 10:35:23,436 - INFO - Task completed, returning run status.\n"
     ]
    }
   ],
   "source": [
    "run_parkings = func.run(action=\"job\",local_execution=True,inputs={\"di\":data_item_download},outputs={\"parkings\":\"parkings\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_parkings = run_parkings.outputs()['parkings'].key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/aggregations_parkings_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/aggregations_parkings_dh_core.py\"\n",
    "from datetime import datetime\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"parking_data_aggregated\"])\n",
    "def aggregate_parkings(project, di):\n",
    "    rdf = di.as_df()\n",
    "    rdf['data'] = pd.to_datetime(rdf['data'])\n",
    "    rdf['day'] = rdf['data'].apply(lambda t: t.replace(second=0, minute=0))\n",
    "    rdf['hour'] = rdf['day'].dt.hour\n",
    "    rdf['dow'] = rdf['day'].dt.dayofweek\n",
    "    #rdf['type'] = rdf['data']#.apply(lambda t: \"sadassad\"+t.astype(str))\n",
    "    rdf['day'] = rdf['day'].apply(lambda t: datetime.timestamp(t)) #added because complain of timestamp not JSOn serializable#\n",
    "    rdf = rdf.drop(columns=['data'])\n",
    "    rdf['lat'] = rdf['lat'].apply(lambda t: float(t))\n",
    "    rdf['lon'] = rdf['lon'].apply(lambda t: float(t))\n",
    "    grouped = rdf.groupby(['parcheggio','day']).mean() #\n",
    "    df_aggregated = grouped.reset_index()\n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"aggregate-parkings\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/aggregations_parkings_dh_core.py\", \"handler\": \"aggregate_parkings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data_item = run.outputs()['dataset'].key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 10:35:23,747 - INFO - Validating task.\n",
      "2024-07-04 10:35:23,747 - INFO - Validating run.\n",
      "2024-07-04 10:35:23,748 - INFO - Starting task.\n",
      "2024-07-04 10:35:23,748 - INFO - Configuring execution.\n",
      "2024-07-04 10:35:23,750 - INFO - Composing function arguments.\n",
      "2024-07-04 10:35:23,792 - INFO - Executing run.\n",
      "2024-07-04 10:35:23,997 - INFO - Task completed, returning run status.\n"
     ]
    }
   ],
   "source": [
    "run_aggregate = func.run(action=\"job\",local_execution=True,inputs={\"di\":data_item_download},outputs={\"parking_data_aggregated\":\"parking_data_aggregated\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'COMPLETED', 'outputs': {'parking_data_aggregated': 'store://parcheggi/dataitems/table/parking_data_aggregated:10150c47-ccbd-48fa-b0f1-17cc88982667'}, 'results': {}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_aggregate.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_aggregate = run_aggregate.outputs()['parking_data_aggregated'].key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcheggio</th>\n",
       "      <th>day</th>\n",
       "      <th>posti_liberi</th>\n",
       "      <th>posti_occupati</th>\n",
       "      <th>posti_totali</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>hour</th>\n",
       "      <th>dow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>1.717722e+09</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>265.0</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>1.717726e+09</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>265.0</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>1.717729e+09</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>265.0</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>1.717733e+09</td>\n",
       "      <td>244.333333</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>265.0</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>1.717736e+09</td>\n",
       "      <td>242.666667</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>265.0</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     parcheggio           day  posti_liberi  posti_occupati  posti_totali  \\\n",
       "0  Autostazione  1.717722e+09    244.000000       21.000000         265.0   \n",
       "1  Autostazione  1.717726e+09    244.000000       21.000000         265.0   \n",
       "2  Autostazione  1.717729e+09    244.000000       21.000000         265.0   \n",
       "3  Autostazione  1.717733e+09    244.333333       20.666667         265.0   \n",
       "4  Autostazione  1.717736e+09    242.666667       22.333333         265.0   \n",
       "\n",
       "         lat        lon  hour  dow  \n",
       "0  44.504422  11.346514   1.0  4.0  \n",
       "1  44.504422  11.346514   2.0  4.0  \n",
       "2  44.504422  11.346514   3.0  4.0  \n",
       "3  44.504422  11.346514   4.0  4.0  \n",
       "4  44.504422  11.346514   5.0  4.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 10:35:24,526 - INFO - Validating task.\n",
      "2024-07-04 10:35:24,527 - INFO - Validating run.\n",
      "2024-07-04 10:35:24,527 - INFO - Starting task.\n",
      "2024-07-04 10:35:24,528 - INFO - Configuring execution.\n",
      "2024-07-04 10:35:24,530 - INFO - Composing function arguments.\n"
     ]
    }
   ],
   "source": [
    "run_aggregate.outputs()['parking_data_aggregated'].as_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digitalhub_owner_user xoMcxHOcwniCoxtBLOsewlzsUD112yeukJHSn69Nj0zs7vKR5HqAaxYQbys51Pnl\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"POSTGRES_USER\"),os.getenv(\"POSTGRES_PASSWORD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/parkings_to_db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/parkings_to_db.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import datetime as dtt\n",
    "import os\n",
    "\n",
    "@handler()\n",
    "def to_db(project, agg_di , parkings_di ):\n",
    "    USERNAME = os.getenv(\"POSTGRES_USER\")#project.get_secret(entity_name='DB_USERNAME').read_secret_value()\n",
    "    PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")#project.get_secret(entity_name='DB_PASSWORD').read_secret_value()\n",
    "    engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/digitalhub')\n",
    "    \n",
    "    agg_df = agg_di.as_df(file_format=\"parquet\")\n",
    "        \n",
    "    # Keep only last two calendar years\n",
    "    date = dtt.date.today() - dtt.timedelta(days=365*2)\n",
    "    agg_df['day'] = agg_df['day'].apply(lambda t: datetime.fromtimestamp(t)) #added because before was converted the type\n",
    "    agg_df = agg_df[agg_df['day'].dt.date >= date]\n",
    "    agg_df.to_sql(\"parking_data_aggregated\", engine, if_exists=\"replace\")\n",
    "    parkings_di.as_df().to_sql('parkings', engine, if_exists=\"replace\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"to-db\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         #requirements=[\"sqlalchemy\"]\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/parkings_to_db.py\", \"handler\": \"to_db\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set secrets\n",
    "#secret_a = proj.new_secret(name=\"DB_USERNAME_NEW\", secret_value=\"digitalhub_owner_user\")\n",
    "#secret_b = proj.new_secret(name=\"DB_PASSWORD\", secret_value=\"secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 11:59:29,649 - INFO - Validating task.\n",
      "2024-07-04 11:59:29,650 - INFO - Validating run.\n",
      "2024-07-04 11:59:29,650 - INFO - Starting task.\n",
      "2024-07-04 11:59:29,650 - INFO - Configuring execution.\n",
      "2024-07-04 11:59:29,652 - INFO - Composing function arguments.\n",
      "2024-07-04 11:59:29,694 - INFO - Executing run.\n",
      "2024-07-04 11:59:29,953 - INFO - Task completed, returning run status.\n"
     ]
    }
   ],
   "source": [
    "#N:B; this might get stuck for some low RAM issues \n",
    "run_to_db = func.run(action=\"job\",local_execution=True,inputs={\"agg_di\":data_item_aggregate,\"parkings_di\":data_item_parkings},outputs={})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_to_db.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_to_db.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install darts==0.25.0 pandas==1.4.4 numpy==1.22.4 patsy==0.5.2 scikit-learn==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataitems = dh.list_dataitems(project=\"parcheggi\")\n",
    "print(dataitems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "dataitem = dh.get_dataitem(project=\"parcheggi\",\n",
    "                           entity_name=\"dataset\")\n",
    "df = dataitem.as_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "window = 60\n",
    "\n",
    "df_clean = df.copy()\n",
    "#print(type(df_clean['data'][0]))\n",
    "#df_clean['data'] = pd.to_datetime(df_clean['data'])\n",
    "df_clean.data = pd.to_datetime(df_clean.data, utc=True)\n",
    "#print(type(df_clean['data'][0]))\n",
    "#df_clean.data = df_clean.data.apply(lambda x: x.to_datetime64()) #.astype('datetime64')\n",
    "df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "#df_clean['date_time_slice'] = df_clean.data.dt.round('30min')\n",
    "df_clean['date_time_slice'] = df_clean.data.dt.round('30min').dt.tz_convert(None)\n",
    "df_clean = df_clean[df_clean.date_time_slice >= (datetime.datetime.today() - pd.DateOffset(window))]\n",
    "df_clean = df_clean[df_clean.date_time_slice <= (datetime.datetime.today() - pd.DateOffset(1))]\n",
    "df_clean.posti_occupati = df_clean.apply(lambda x: max(0, min(x['posti_totali'], x['posti_occupati'])), axis=1)\n",
    "df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "df_clean = df_clean.drop(columns=['lat', 'lon', 'data', 'posti_totali', 'posti_liberi', 'posti_occupati'])\n",
    "df_clean#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "split_ratio = 0.8\n",
    "\n",
    "def fill_missing(parc_df):\n",
    "    missing = []  # List to store timestamps for which values could not be filled\n",
    "    temp = pd.Series(parc_df.index.date).value_counts()  # Count the occurrences of each date\n",
    "    temp = temp[temp < 48]  # Filter dates with less than 48 occurrences\n",
    "    temp.sort_index(inplace=True)  # Sort the dates in ascending order\n",
    "    for t in temp.index:  # Iterate through the filtered dates\n",
    "        for h in range(24):  # Iterate through 24 hours\n",
    "            for half_hour in [0, 30]:  # Iterate through 0 and 30 minutes\n",
    "                ts = datetime.datetime(t.year, t.month, t.day, h, half_hour)  # Create a timestamp\n",
    "                if ts not in parc_df.index:  # If the timestamp is missing in the DataFrame\n",
    "                    if ts - datetime.timedelta(days=7) in parc_df.index:  # Check if the previous week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts - datetime.timedelta(days=7)].copy()  # Copy values from the previous week\n",
    "                    elif ts + datetime.timedelta(days=7) in parc_df.index:  # Check if the next week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts + datetime.timedelta(days=7)].copy()  # Copy values from the next week\n",
    "                    else:\n",
    "                        missing.append(ts)  # If values cannot be filled, add the timestamp to the missing list\n",
    "    return missing \n",
    "\n",
    "\n",
    "def split_dataset(df_clean):\n",
    "    parcheggi = df_clean['parcheggio'].unique()\n",
    "    train_sets, val_sets = [], []\n",
    "\n",
    "    for parcheggio in parcheggi:\n",
    "        parc_df = df_clean[df_clean['parcheggio'] == parcheggio]\n",
    "        parc_df['hour'] = parc_df.date_time_slice.dt.hour\n",
    "        parc_df['dow'] = parc_df.date_time_slice.dt.dayofweek\n",
    "        parc_df = parc_df.drop(columns=['parcheggio'])\n",
    "        parc_df = parc_df.groupby('date_time_slice').agg({'occupied': 'mean', 'hour': 'first', 'dow': 'first'})\n",
    "        fill_missing(parc_df)\n",
    "        ts = TimeSeries.from_dataframe(parc_df,  value_cols='occupied', freq='30min')\n",
    "        ts_scaled = Scaler().fit_transform(ts)\n",
    "\n",
    "        split = int(len(ts_scaled) * (1 - split_ratio))\n",
    "\n",
    "        train, val = ts_scaled[:-split], ts_scaled[-split:]\n",
    "        train_sets.append(train)\n",
    "        val_sets.append(val)\n",
    "    return train_sets,val_sets\n",
    "train_sets, val_sets = split_dataset(df_clean)    \n",
    "train_sets[0].plot(label='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import NBEATSModel\n",
    "\n",
    "multimodel =  NBEATSModel(\n",
    "        input_chunk_length=24,\n",
    "        output_chunk_length=12,\n",
    "        n_epochs=10,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "multimodel.fit(train_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred = multimodel.predict(n=24, series=train_sets[0][:-24])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_sets[0].plot(label=\"actual\")\n",
    "pred.plot(label=\"forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mape, smape, mae\n",
    "\n",
    "metrics = {\n",
    "    \"mape\": mape(train_sets[0], pred),\n",
    "    \"smape\": smape(train_sets[0], pred),\n",
    "    \"mae\": mae(train_sets[0], pred)\n",
    "}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "PROJECT_NAME = \"MLparcheggi\"\n",
    "ml_proj = dh.get_or_create_project(PROJECT_NAME) # source=\"git://github.com/scc-digitalhub/gdb-project-parkings.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/train_multimodel.py\"\n",
    "\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "from darts import TimeSeries\n",
    "\n",
    "from darts.models import NBEATSModel\n",
    "from darts.metrics import mape, smape, mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pickle import dumps\n",
    "\n",
    "def fill_missing(parc_df):\n",
    "    missing = []  # List to store timestamps for which values could not be filled\n",
    "    temp = pd.Series(parc_df.index.date).value_counts()  # Count the occurrences of each date\n",
    "    temp = temp[temp < 48]  # Filter dates with less than 48 occurrences\n",
    "    temp.sort_index(inplace=True)  # Sort the dates in ascending order\n",
    "    for t in temp.index:  # Iterate through the filtered dates\n",
    "        for h in range(24):  # Iterate through 24 hours\n",
    "            for half_hour in [0, 30]:  # Iterate through 0 and 30 minutes\n",
    "                ts = datetime.datetime(t.year, t.month, t.day, h, half_hour)  # Create a timestamp\n",
    "                if ts not in parc_df.index:  # If the timestamp is missing in the DataFrame\n",
    "                    if ts - datetime.timedelta(days=7) in parc_df.index:  # Check if the previous week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts - datetime.timedelta(days=7)].copy()  # Copy values from the previous week\n",
    "                    elif ts + datetime.timedelta(days=7) in parc_df.index:  # Check if the next week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts + datetime.timedelta(days=7)].copy()  # Copy values from the next week\n",
    "                    else:\n",
    "                        missing.append(ts)  # If values cannot be filled, add the timestamp to the missing list\n",
    "    return missing  # Return the list of timestamps for which values could not be filled\n",
    "\n",
    "\n",
    "@handler()\n",
    "def train_model(project, parkings_di,n_epochs: int = 1, window: int = 60, \n",
    "                input_chunk_length: int = 24, output_chunk_length: int = 12, \n",
    "                split_ratio: float = 0.8):\n",
    "\n",
    "    # Load the input data\n",
    "    df_source = parkings_di.as_df()\n",
    "    # Clean the data\n",
    "    df_clean = df_source.copy()\n",
    "    df_clean.data = pd.to_datetime(df_clean.data, utc=True)\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "    df_clean['date_time_slice'] = df_clean.data.dt.round('30min').dt.tz_convert(None)\n",
    "    df_clean = df_clean[df_clean.date_time_slice >= (datetime.datetime.today() - pd.DateOffset(window))]\n",
    "    df_clean = df_clean[df_clean.date_time_slice <= (datetime.datetime.today() - pd.DateOffset(1))]\n",
    "    df_clean.posti_occupati = df_clean.apply(lambda x: max(0, min(x['posti_totali'], x['posti_occupati'])), axis=1)\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "    df_clean = df_clean.drop(columns=['lat', 'lon', 'data', 'posti_totali', 'posti_liberi', 'posti_occupati'])\n",
    "    parcheggi = df_clean['parcheggio'].unique()\n",
    "\n",
    "    train_sets, val_sets = [], []\n",
    "\n",
    "    # Process data for each parking lot\n",
    "    for parcheggio in parcheggi:\n",
    "        parc_df = df_clean[df_clean['parcheggio'] == parcheggio]\n",
    "        parc_df['hour'] = parc_df.date_time_slice.dt.hour\n",
    "        parc_df['dow'] = parc_df.date_time_slice.dt.dayofweek\n",
    "        parc_df = parc_df.drop(columns=['parcheggio'])\n",
    "        parc_df = parc_df.groupby('date_time_slice').agg({'occupied': 'mean', 'hour': 'first', 'dow': 'first'})\n",
    "        fill_missing(parc_df)\n",
    "        ts = TimeSeries.from_dataframe(parc_df,  value_cols='occupied', freq='30min')\n",
    "        ts_scaled = Scaler().fit_transform(ts)\n",
    "        \n",
    "        split = int(len(ts_scaled) * (1 - split_ratio))\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        train, val = ts_scaled[:-split], ts_scaled[-split:]\n",
    "        train_sets.append(train)\n",
    "        val_sets.append(val)\n",
    "\n",
    "    # Train a multi-model using the NBEATS algorithm\n",
    "    multimodel =  NBEATSModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=output_chunk_length,\n",
    "        n_epochs=n_epochs,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Fit the model to the training sets\n",
    "    multimodel.fit(train_sets)\n",
    "    pred = multimodel.predict(n=output_chunk_length*2, series=train_sets[0][:-output_chunk_length*2])\n",
    "\n",
    "    multimodel.save(\"parcheggi_predictor_model.pt\")\n",
    "    with ZipFile(\"parcheggi_predictor_model.pt.zip\", \"w\") as z:\n",
    "        z.write(\"parcheggi_predictor_model.pt\")\n",
    "        z.write(\"parcheggi_predictor_model.pt.ckpt\")\n",
    "\n",
    "    #project.log_model(\n",
    "    #    \"parcheggi_predictor_model\",\n",
    "    #    kind=\"model\",\n",
    "    #    parameters={\n",
    "    #        \"window\": window,\n",
    "    #        \"input_chunk_length\": input_chunk_length,\n",
    "    #        \"output_chunk_length\": output_chunk_length,\n",
    "    #        \"n_epochs\": n_epochs,\n",
    "    #        \"split_ratio\": split_ratio,\n",
    "    #        \"num_layers\": multimodel.num_layers,\n",
    "    #        \"layer_widths\": multimodel.layer_widths,\n",
    "    #        \"activation\": multimodel.activation\n",
    "    #    },\n",
    "    #    metrics = {\n",
    "    #        \"mape\": mape(train_sets[0], pred),\n",
    "    #        \"smape\": smape(train_sets[0], pred),\n",
    "    #        \"mae\": mae(train_sets[0], pred)\n",
    "    #    },\n",
    "    #    model_file=\"parcheggi_predictor_model.pt.zip\",\n",
    "    #    labels={\"class\": \"darts.models.NBEATSModel\"},\n",
    "    #    algorithm=\"darts.models.NBEATSModel\",\n",
    "    #    framework=\"darts\"\n",
    "    #) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"training_model\"\n",
    "func = ml_proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         #requirements =[\"darts==0.25.0\", \"pandas==1.4.4\", \"numpy==1.22.4\", \"patsy==0.5.2\", \"scikit-learn==1.1.2\"],\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/train_multimodel.py\", \"handler\": \"train_model\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download # relative to the parkings above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train_model = func.run(action=\"job\",local_execution=True,inputs={\"parkings_di\":data_item_download},outputs={})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitalhub-core",
   "language": "python",
   "name": "digitalhub-core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
