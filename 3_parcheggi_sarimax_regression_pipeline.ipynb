{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4841224-7977-4527-9b35-eb2d8aa08a69",
   "metadata": {},
   "source": [
    "# Platform Support - Data Ops\r\n",
    "We use the platform support to read the data created into the platform after the execution of notebook(parcheggi_data_pipeline.ipynb) for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07163ddc-42ce-48f7-b56b-a493910b732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89067f39-04ee-4568-a725-b2e68df6cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/exports/csv?lang=it&timezone=UTC&use_labels=true&delimiter=%3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c00062-dc9f-47a3-9241-899273109de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created project parcheggi-nk-scheduler-digitalhubdev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'parcheggi-nk-scheduler-digitalhubdev'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import digitalhub as dh\n",
    "import getpass as gt\n",
    "\n",
    "PROJECT_NAME = \"parcheggi-nk-scheduler-\"+gt.getuser()\n",
    "proj = dh.get_or_create_project(PROJECT_NAME)\n",
    "print(\"created project {}\".format(PROJECT_NAME))\n",
    "PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295dd1fe-787f-4105-ba4c-2f8e59ee1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory=\"src\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb1bf0-5193-49a1-b6d4-ff39d2c59dcc",
   "metadata": {},
   "source": [
    "# Predict day (Regression SARIMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cdc715b-82b4-442f-a95a-adbea87b0412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/predict_sarimax_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/predict_sarimax_regression.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define a custom function to serialize datetime objects \n",
    "def serialize_datetime(obj): \n",
    "    if isinstance(obj, datetime.datetime): \n",
    "        return obj.isoformat() \n",
    "    raise TypeError(\"Type not serializable\") \n",
    "\n",
    "    \n",
    "def to_point(point):\n",
    "    \"\"\"\n",
    "    Convert a decimal number representing minutes to a datetime object.\n",
    "\n",
    "    Args:\n",
    "        point (float): The decimal number representing minutes.\n",
    "\n",
    "    Returns:\n",
    "        datetime.datetime: A datetime object representing the current date and time with the minutes derived from the input.\n",
    "\n",
    "    Example:\n",
    "        >>> to_point(45)\n",
    "        datetime.datetime(2022, 1, 1, 0, 22, 30)\n",
    "    \"\"\"\n",
    "    today = datetime.datetime.today()\n",
    "    #return datetime.datetime(today.year, today.month, today.day, int(point * 30 / 60), int(point * 30 % 60))\n",
    "    dt = datetime.datetime(today.year, today.month, today.day, int(point * 30 / 60), int(point * 30 % 60))\n",
    "    return json.dumps(dt, default=serialize_datetime)\n",
    "\n",
    "\n",
    "@handler(outputs=[\"parking_data_predicted_regression\"])\n",
    "def predict_day(project, parkings_di):\n",
    "    \"\"\"\n",
    "    Predicts the occupancy of parking spaces for the next 48 steps and saves the results in a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        parkings_di: The data item containing the parking data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert data item to pandas DataFrame\n",
    "    df = parkings_di.as_df()\n",
    "\n",
    "    # Create a clean copy of the DataFrame\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    df_clean = df_clean.drop(columns=['lat', 'lon'])\n",
    "\n",
    "    # Convert 'data' column to datetime\n",
    "    df_clean.data = df_clean.data.astype('datetime64[ms, UTC]')\n",
    "\n",
    "    # Calculate the occupancy rate\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "\n",
    "    # Round the 'data' column to the nearest 30 minutes\n",
    "    df_clean['date_time_slice'] = df_clean.data.dt.round('30min').dt.tz_convert(None)\n",
    "\n",
    "    # Extract the date from the 'data' column\n",
    "    df_clean['date'] = df_clean.data.dt.tz_convert(None)\n",
    "    # df_clean['date'] = df_clean['date'].tz_convert(None)\n",
    "\n",
    "    # Filter out data from the last 30 days\n",
    "    df_clean = df_clean[df_clean.date_time_slice >= (datetime.datetime.today() - pd.DateOffset(30))]\n",
    "\n",
    "    # Filter out data from today\n",
    "    df_clean = df_clean[df_clean.date <= (datetime.datetime.today() - pd.DateOffset(1))]\n",
    "\n",
    "    # Remove the 'date' column\n",
    "    df_clean = df_clean.drop(['date'], axis=1)\n",
    "\n",
    "    # Ensure that 'posti_occupati' is within the range of [0, posti_totali]\n",
    "    df_clean.posti_occupati = df_clean.apply(lambda x: max(0, min(x['posti_totali'], x['posti_occupati'])), axis=1)\n",
    "\n",
    "    # Recalculate the occupancy rate\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "\n",
    "    # Get unique parking locations\n",
    "    parcheggi = df_clean['parcheggio'].unique()\n",
    "    # parcheggi = ['Riva Reno' ,'VIII Agosto']\n",
    "\n",
    "    # Initialize a list to store the predictions\n",
    "    res = []\n",
    "\n",
    "    # Iterate over each parking location\n",
    "    for parcheggio in parcheggi:\n",
    "        # Create a copy of the cleaned DataFrame\n",
    "        cp = df_clean.copy()\n",
    "\n",
    "        # Filter data for the current parking location\n",
    "        parc_df = cp[cp['parcheggio'] == parcheggio]\n",
    "\n",
    "        # Group data by 'date_time_slice' and aggregate metrics\n",
    "        parc_df = parc_df.groupby('date_time_slice').agg({'posti_occupati':['sum','count'], 'posti_totali':['sum','count']})\n",
    "\n",
    "        # Calculate the occupancy rate\n",
    "        parc_df['occupied'] = parc_df.posti_occupati['sum'] / parc_df.posti_totali['sum']\n",
    "\n",
    "        # Remove unnecessary columns\n",
    "        parc_df.drop(columns=['posti_occupati', 'posti_totali'], inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by index\n",
    "        parc_df.sort_index(inplace=True)\n",
    "\n",
    "        # Extract the 'occupied' column as a Series\n",
    "        data = parc_df.reset_index()['occupied']\n",
    "\n",
    "        # Define the SARIMA model parameters\n",
    "        my_seasonal_order = (1, 1, 1, 48)\n",
    "\n",
    "        # Create and fit the SARIMA model\n",
    "        sarima_model = SARIMAX(data, order=(1, 0, 1), seasonal_order=my_seasonal_order)\n",
    "        results_SAR = sarima_model.fit(disp=-1)\n",
    "\n",
    "        # Generate predictions for the next 48 steps\n",
    "        pred = results_SAR.forecast(steps=48).reset_index()\n",
    "\n",
    "        # Add the 'parcheggio' column\n",
    "        pred['parcheggio'] = parcheggio\n",
    "        res.append(pred)\n",
    "    \n",
    "    for pred in res:\n",
    "        pred['point'] = (pred.index).astype('int')\n",
    "        pred['datetime'] = pred['point'].apply(to_point)\n",
    "        pred.drop(['point'], axis=1, inplace=True)\n",
    "    \n",
    "    all = pd.concat(res, ignore_index=True)[['predicted_mean', 'parcheggio', 'datetime']]\n",
    "\n",
    "    USERNAME = os.getenv(\"POSTGRES_USER\")\n",
    "    PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "    engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/digitalhub')\n",
    "    with engine.connect() as connection: \n",
    "        try: connection.execute(\"DELETE FROM parkings_prediction\")\n",
    "        except: pass\n",
    "\n",
    "    all.to_sql('parkings_prediction', engine, if_exists=\"append\")\n",
    "\n",
    "    # old_pd = all\n",
    "    # try: \n",
    "    #     dat_old = project.get_dataitem('parking_prediction_sarima_model')\n",
    "    #     old_pd = pd.concat([dat_old.as_df(), all], ignore_index=True)\n",
    "    # except: pass\n",
    "    # project.log_dataitem(name='parking_prediction_sarima_model', data=old_pd, kind=\"table\")\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0167c5e-f105-46ac-b61f-05062a990ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = proj.new_function(name=\"predict-day-sarimax-regression\",\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/predict_sarimax_regression.py\", \"handler\": \"predict_day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccd395e-9d70-47bd-8391-93b8a6e4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download = proj.get_dataitem(\"dataset\").key\n",
    "run_parkings = func.run(action=\"job\",inputs={\"parkings_di\":data_item_download},outputs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66add694-7e6a-42a3-8510-65c78256d804",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc271c-16a1-462e-ba6d-774518c1f7fd",
   "metadata": {},
   "source": [
    "In this step we will create a workflow pipeline whose purpose is to call the download and predict-day(regression) function based on schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a09968f-bf89-4077-9bf1-37785054e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/parkings_pipeline_sarimax_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/parkings_pipeline_sarimax_regression.py\"\n",
    "\n",
    "from digitalhub_runtime_kfp.dsl import pipeline_context\n",
    "\n",
    "def myhandler(url):\n",
    "    with pipeline_context() as pc:\n",
    "        s1_dataset = pc.step(name=\"download\", function=\"downloader-funct\", action=\"job\", inputs={\"url\":url},outputs={\"dataset\":\"dataset\"})\n",
    "        s2_predict = pc.step(name=\"predict-day-sarimax-regression\", function=\"predict-day-sarimax-regression\", action=\"job\", inputs={\"parkings_di\":s1_dataset.outputs['dataset']}, outputs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00957ebb-4ac5-4aed-a945-0969cde9bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = proj.new_workflow(name=\"pipeline_parcheggi_sarimax_regression\", kind=\"kfp\", source={\"source\": \"src/parkings_pipeline_sarimax_regression.py\", \"handler\": \"myhandler\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5724c9a-24a8-4dbc-9967-69ec552ad099",
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n",
    "c = workflow.run(parameters={\"url\": di.key})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57888a2b-f8fa-4195-a2c0-8295a9d9702b",
   "metadata": {},
   "source": [
    "## Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b2242-61e6-46ff-8a9d-6bb330d4dfe5",
   "metadata": {},
   "source": [
    "Regression Pipeline workflow is scheduled for frequent runs using Crons expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2d4e117-d111-433a-84f6-5fa6e66b1118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'kfp+run', 'metadata': {'project': 'parcheggi-nk-scheduler-digitalhubdev', 'name': 'f82d0f78-9040-48eb-84d8-282d884e849d', 'created': '2024-09-30T11:51:27.291Z', 'updated': '2024-09-30T11:51:27.305Z', 'created_by': 'tenant1userid', 'updated_by': 'tenant1userid'}, 'spec': {'task': 'kfp+pipeline://parcheggi-nk-scheduler-digitalhubdev/pipeline_parcheggi_regression:f7c7d060-4903-4660-9943-f38636ca195f', 'local_execution': False, 'source': {'source': 'src/parkings_pipeline_regression.py', 'handler': 'myhandler', 'base64': 'CmZyb20gZGlnaXRhbGh1Yl9ydW50aW1lX2tmcC5kc2wgaW1wb3J0IHBpcGVsaW5lX2NvbnRleHQKCmRlZiBteWhhbmRsZXIodXJsKToKICAgIHdpdGggcGlwZWxpbmVfY29udGV4dCgpIGFzIHBjOgogICAgICAgIHMxX2RhdGFzZXQgPSBwYy5zdGVwKG5hbWU9ImRvd25sb2FkIiwgZnVuY3Rpb249ImRvd25sb2FkZXItZnVuY3QiLCBhY3Rpb249ImpvYiIsIGlucHV0cz17InVybCI6dXJsfSxvdXRwdXRzPXsiZGF0YXNldCI6ImRhdGFzZXQifSkKICAgICAgICBzMl9wcmVkaWN0ID0gcGMuc3RlcChuYW1lPSJwcmVkaWN0IiwgZnVuY3Rpb249InByZWRpY3QtZGF5IiwgYWN0aW9uPSJqb2IiLCBpbnB1dHM9eyJwYXJraW5nc19kaSI6czFfZGF0YXNldC5vdXRwdXRzWydkYXRhc2V0J119LCBvdXRwdXRzPXt9KQo=', 'lang': 'python'}, 'function': 'kfp://parcheggi-nk-scheduler-digitalhubdev/pipeline_parcheggi_regression:f7c7d060-4903-4660-9943-f38636ca195f', 'schedule': '@hourly', 'inputs': {}, 'outputs': {}, 'parameters': {'url': 'store://parcheggi-nk-scheduler-digitalhubdev/dataitem/table/url_data_item:f364c2eb-75ee-41a9-9b64-c2c50c04d60e'}}, 'status': {'state': 'READY', 'transitions': [{'status': 'READY', 'time': '2024-09-30T11:51:27.30575017Z'}, {'status': 'BUILT', 'time': '2024-09-30T11:51:27.296232355Z'}]}, 'user': 'tenant1userid', 'project': 'parcheggi-nk-scheduler-digitalhubdev', 'id': 'f82d0f78-9040-48eb-84d8-282d884e849d', 'key': 'store://parcheggi-nk-scheduler-digitalhubdev/run/kfp+run/f82d0f78-9040-48eb-84d8-282d884e849d'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.run(parameters={\"url\": di.key}, schedule=\"@hourly\")\n",
    "#workflow.run(parameters={\"url\": di.key}, schedule=\"*/5 * * * *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55dd2f-7111-40e9-8247-f37652522eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
