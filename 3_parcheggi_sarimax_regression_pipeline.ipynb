{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4841224-7977-4527-9b35-eb2d8aa08a69",
   "metadata": {},
   "source": [
    "# Platform Support - Data Ops\n",
    "We use the platform support to read the data created into the platform after the execution of notebook(parcheggi_data_pipeline.ipynb) for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07163ddc-42ce-48f7-b56b-a493910b732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89067f39-04ee-4568-a725-b2e68df6cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/exports/csv?lang=it&timezone=UTC&use_labels=true&delimiter=%3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c00062-dc9f-47a3-9241-899273109de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "PROJECT_NAME = \"parcheggi-scheduler\"\n",
    "proj = dh.get_or_create_project(PROJECT_NAME)\n",
    "print(\"created project {}\".format(PROJECT_NAME))\n",
    "PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295dd1fe-787f-4105-ba4c-2f8e59ee1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory=\"src\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb1bf0-5193-49a1-b6d4-ff39d2c59dcc",
   "metadata": {},
   "source": [
    "# Predict day (Regression SARIMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc715b-82b4-442f-a95a-adbea87b0412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile \"src/predict_sarimax_regression.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define a custom function to serialize datetime objects \n",
    "def serialize_datetime(obj): \n",
    "    if isinstance(obj, datetime.datetime): \n",
    "        return obj.isoformat() \n",
    "    raise TypeError(\"Type not serializable\") \n",
    "\n",
    "def to_point(point):\n",
    "    \"\"\"\n",
    "    Convert a decimal number representing minutes to a datetime object.\n",
    "\n",
    "    Args:\n",
    "        point (float): The decimal number representing minutes.\n",
    "\n",
    "    Returns:\n",
    "        datetime.datetime: A datetime object representing the current date and time with the minutes derived from the input.\n",
    "\n",
    "    Example:\n",
    "        >>> to_point(45)\n",
    "        datetime.datetime(2022, 1, 1, 0, 22, 30)\n",
    "    \"\"\"\n",
    "    today = datetime.datetime.today()\n",
    "    #return datetime.datetime(today.year, today.month, today.day, int(point * 30 / 60), int(point * 30 % 60))\n",
    "    dt = datetime.datetime(today.year, today.month, today.day, int(point * 30 / 60), int(point * 30 % 60))\n",
    "    return dt;\n",
    "    # return json.dumps(dt, default=serialize_datetime)\n",
    "\n",
    "\n",
    "@handler()\n",
    "def predict_day(project, parkings_di):\n",
    "    \"\"\"\n",
    "    Predicts the occupancy of parking spaces for the next 48 steps and saves the results in a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        parkings_di: The data item containing the parking data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert data item to pandas DataFrame\n",
    "    df = parkings_di.as_df()\n",
    "\n",
    "    # Create a clean copy of the DataFrame\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    df_clean = df_clean.drop(columns=['lat', 'lon'])\n",
    "\n",
    "    # Convert 'data' column to datetime\n",
    "    df_clean.data = df_clean.data.astype('datetime64[ms, UTC]')\n",
    "\n",
    "    # Calculate the occupancy rate\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "\n",
    "    # Round the 'data' column to the nearest 30 minutes\n",
    "    df_clean['date_time_slice'] = df_clean.data.dt.round('30min').dt.tz_convert(None)\n",
    "\n",
    "    # Extract the date from the 'data' column\n",
    "    df_clean['date'] = df_clean.data.dt.tz_convert(None)\n",
    "    # df_clean['date'] = df_clean['date'].tz_convert(None)\n",
    "\n",
    "    # Filter out data from the last 30 days\n",
    "    df_clean = df_clean[df_clean.date_time_slice >= (datetime.datetime.today() - pd.DateOffset(30))]\n",
    "\n",
    "    # Filter out data from today\n",
    "    df_clean = df_clean[df_clean.date <= (datetime.datetime.today() - pd.DateOffset(1))]\n",
    "\n",
    "    # Remove the 'date' column\n",
    "    df_clean = df_clean.drop(['date'], axis=1)\n",
    "\n",
    "    # Ensure that 'posti_occupati' is within the range of [0, posti_totali]\n",
    "    df_clean.posti_occupati = df_clean.apply(lambda x: max(0, min(x['posti_totali'], x['posti_occupati'])), axis=1)\n",
    "\n",
    "    # Recalculate the occupancy rate\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "\n",
    "    # Get unique parking locations\n",
    "    parcheggi = df_clean['parcheggio'].unique()\n",
    "    # parcheggi = ['Riva Reno' ,'VIII Agosto']\n",
    "\n",
    "    # Initialize a list to store the predictions\n",
    "    res = []\n",
    "\n",
    "    # Iterate over each parking location\n",
    "    for parcheggio in parcheggi:\n",
    "        # Create a copy of the cleaned DataFrame\n",
    "        cp = df_clean.copy()\n",
    "\n",
    "        # Filter data for the current parking location\n",
    "        parc_df = cp[cp['parcheggio'] == parcheggio]\n",
    "\n",
    "        # Group data by 'date_time_slice' and aggregate metrics\n",
    "        parc_df = parc_df.groupby('date_time_slice').agg({'posti_occupati':['sum','count'], 'posti_totali':['sum','count']})\n",
    "\n",
    "        # Calculate the occupancy rate\n",
    "        parc_df['occupied'] = parc_df.posti_occupati['sum'] / parc_df.posti_totali['sum']\n",
    "\n",
    "        # Remove unnecessary columns\n",
    "        parc_df.drop(columns=['posti_occupati', 'posti_totali'], inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by index\n",
    "        parc_df.sort_index(inplace=True)\n",
    "\n",
    "        # Extract the 'occupied' column as a Series\n",
    "        data = parc_df.reset_index()['occupied']\n",
    "\n",
    "        # Define the SARIMA model parameters\n",
    "        my_seasonal_order = (1, 1, 1, 48)\n",
    "\n",
    "        # Create and fit the SARIMA model\n",
    "        sarima_model = SARIMAX(data, order=(1, 0, 1), seasonal_order=my_seasonal_order)\n",
    "        results_SAR = sarima_model.fit(disp=-1)\n",
    "\n",
    "        # Generate predictions for the next 48 steps\n",
    "        pred = results_SAR.forecast(steps=48).reset_index()\n",
    "\n",
    "        # Add the 'parcheggio' column\n",
    "        pred['parcheggio'] = parcheggio\n",
    "        res.append(pred)\n",
    "    \n",
    "    for pred in res:\n",
    "        pred['point'] = (pred.index).astype('int')\n",
    "        pred['datetime'] = pred['point'].apply(to_point)\n",
    "        pred.drop(['point'], axis=1, inplace=True)\n",
    "    \n",
    "    all = pd.concat(res, ignore_index=True)[['predicted_mean', 'parcheggio', 'datetime']]\n",
    "    \n",
    "    USERNAME = os.getenv(\"DB_USERNAME\")\n",
    "    PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "    DATABASE = os.getenv(\"DB_DATABASE\")\n",
    "    engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/'+DATABASE)\n",
    "    with engine.connect() as connection: \n",
    "        try: connection.execute(\"DELETE FROM parkings_prediction\")\n",
    "        except: pass\n",
    "\n",
    "    all.to_sql('parkings_prediction', engine, if_exists=\"append\")\n",
    "\n",
    "    # old_pd = all\n",
    "    # try: \n",
    "    #     dat_old = project.get_dataitem('parking_prediction_sarima_model')\n",
    "    #     old_pd = pd.concat([dat_old.as_df(), all], ignore_index=True)\n",
    "    # except: pass\n",
    "    # project.log_dataitem(name='parking_prediction_sarima_model', data=old_pd, kind=\"table\")\n",
    "    # return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0167c5e-f105-46ac-b61f-05062a990ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = proj.new_function(name=\"predict-day-sarimax-regression\",\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_10\",\n",
    "                         source={\"source\": \"src/predict_sarimax_regression.py\", \"handler\": \"predict_day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8bcb1-bb65-4692-a724-26b505b959fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_build = func.run(action=\"build\", instructions=[\"pip3 install statsmodels patsy==0.5.6\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ecf95-5af1-4389-af73-205525fc7c73",
   "metadata": {},
   "source": [
    "Wait until build run is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d56520-4c1d-450a-b976-b0fbce783c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_build.refresh().status.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd395e-9d70-47bd-8391-93b8a6e4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download = proj.get_dataitem(\"dataset\").key\n",
    "run_parkings = func.run(action=\"job\",inputs={\"parkings_di\":data_item_download},outputs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d2c20-2481-45ea-a1db-a4777a6a331a",
   "metadata": {},
   "source": [
    "Wait until Sarimax model run is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66375859-4621-43aa-8abe-f9a6c4290462",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_parkings.refresh().status.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26477b-1dc1-48d0-9df6-030e5f02ab96",
   "metadata": {},
   "source": [
    "Once 'Completed', proceed to next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66add694-7e6a-42a3-8510-65c78256d804",
   "metadata": {},
   "source": [
    "# 2.3 Data Management Pipeline\n",
    "We create a data management pipeline that executes the data management functions in the platform. The following pipeline workflow depends on the functions created in Step 1 of scenario.\n",
    "In this step we will create a workflow pipeline whose purpose is to call the download and predict-day(regression) function based on schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09968f-bf89-4077-9bf1-37785054e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/data_management_parkings_pipeline.py\"\n",
    "\n",
    "from digitalhub_runtime_kfp.dsl import pipeline_context\n",
    "\n",
    "def myhandler(url):\n",
    "    with pipeline_context() as pc:\n",
    "        s1_dataset = pc.step(name=\"download\", function=\"downloader-funct\", action=\"job\",inputs={\"url\":url},outputs={\"dataset\":\"dataset\"})        \n",
    "        s2_parking = pc.step(name=\"extract_parking\", function=\"extract-parkings\", action=\"job\",inputs={\"di\":s1_dataset.outputs['dataset']},outputs={\"parkings\":\"parkings\"})        \n",
    "        s3_aggregate = pc.step(name=\"aggregate\",  function=\"aggregate-parkings\", action=\"job\",inputs={\"di\":s1_dataset.outputs['dataset']},outputs={\"parking_data_aggregated\":\"parking_data_aggregated\"})        \n",
    "        s4_predict = pc.step(name=\"predict-day-sarimax-regression\", function=\"predict-day-sarimax-regression\", action=\"job\", inputs={\"parkings_di\":s1_dataset.outputs['dataset']}, outputs={})\n",
    "        s5_to_db = pc.step(name=\"to_db\",  function=\"to-db\", action=\"job\",inputs={\"agg_di\": s3_aggregate.outputs['parking_data_aggregated'],\"parkings_di\":s2_parking.outputs['parkings']},outputs={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00957ebb-4ac5-4aed-a945-0969cde9bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = proj.new_workflow(name=\"pipeline_sarimax_regression\", kind=\"kfp\", code_src= \"src/data_management_parkings_pipeline.py\", handler = \"myhandler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22458987-12c9-42da-8d7f-66d0431e129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run(action=\"build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80bfc1-b065-410a-afec-fbdedc8056d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5724c9a-24a8-4dbc-9967-69ec552ad099",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_run = workflow.run(action=\"pipeline\", parameters={\"url\": di.key})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57888a2b-f8fa-4195-a2c0-8295a9d9702b",
   "metadata": {},
   "source": [
    "## Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b2242-61e6-46ff-8a9d-6bb330d4dfe5",
   "metadata": {},
   "source": [
    "Regression Pipeline workflow is scheduled for frequent runs using Crons expression. We scheduled a routine to  fetch data and run regerssion at midnight daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4e117-d111-433a-84f6-5fa6e66b1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run(parameters={\"url\": di.key}, schedule=\" 0 * * *\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
