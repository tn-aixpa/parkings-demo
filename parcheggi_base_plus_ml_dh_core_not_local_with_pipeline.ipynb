{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "### 1.1. Download data\n",
    "Download data from the API, and load it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/exports/csv?lang=it&timezone=UTC&use_labels=true&delimiter=%3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Platform Support - Data Ops\n",
    "\n",
    "We use the platform support to load the data into the platform, version it, and automate the execution of the data management operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initalization\n",
    "Create the working context: data management project for the parking data processing. Project is a placeholder for the code, data, and management of the parking data operations. To keep it reproducible, we use the `git` source type to store the definition and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_funtion_complete(rrun,seconds = 1,status =False):\n",
    "    import time\n",
    "    rrun.refresh()\n",
    "    state = rrun.status\n",
    "    #print(state)\n",
    "    v = 0\n",
    "    while state.state =='READY' or state.state =='RUNNING':\n",
    "        print(f\"Seconds: {v}, {'status: '+ state.state if status else ''} ...\")\n",
    "        v+=seconds\n",
    "        rrun.refresh()\n",
    "        state = rrun.status\n",
    "        time.sleep(seconds)\n",
    "    print(f\"Finished at {v} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "PROJECT_NAME = \"parcheggi_not_local\"\n",
    "proj = dh.get_or_create_project(PROJECT_NAME) # source=\"git://github.com/scc-digitalhub/gdb-project-parkings.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data management functions\n",
    "We convert the data management ETL operations into functions - single executable operations that can be executed in the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting download_all_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"download_all_dh_core.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"dataset\"])\n",
    "def downloader(project, url):\n",
    "    df = url.as_df(file_format='csv',sep=\";\")\n",
    "    df[['lat', 'lon']] = df['coordinate'].str.split(', ',expand=True)\n",
    "    df = df.drop(columns=['% occupazione', 'GUID', 'coordinate']).rename(columns={'Parcheggio': 'parcheggio', 'Data': 'data', 'Posti liberi': 'posti_liberi', 'Posti occupati': 'posti_occupati', 'Posti totali': 'posti_totali'})\n",
    "    df[\"lat\"] = pd.to_numeric(df[\"lat\"])\n",
    "    df[\"lon\"] = pd.to_numeric(df[\"lon\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"downloader-funct\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"download_all_dh_core.py\", \"handler\": \"downloader\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download = func.run(action=\"job\",inputs={\"url\":di.key},outputs={\"dataset\":\"dataset\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Seconds: 60,  ...\n",
      "Seconds: 70,  ...\n",
      "Seconds: 80,  ...\n",
      "Finished at 90 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_download,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_download.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://parcheggi_not_local/dataitems/table/dataset:32d5185b-e56c-4b4f-98ed-10041836a197'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_download  = dh.get_dataitem(project=PROJECT_NAME,entity_name=\"dataset\").key\n",
    "data_item_download  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_item_download = proj.new_dataitem(name=\"dataset\", kind=\"table\", path=\"s3://datalake/parcheggi/dataitems/f2024e9f-6dda-4a77-9216-80713b881300/data.parquet\")#run_download.outputs()['dataset'].key\n",
    "#data_item_download = data_item_download.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting extract_parkings_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"extract_parkings_dh_core.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"parkings\"])\n",
    "def extract_parkings(project, di):\n",
    "    KEYS = ['parcheggio', 'lat', 'lon', 'posti_totali']\n",
    "    df_parcheggi = di.as_df().groupby(['parcheggio']).first().reset_index()[KEYS]\n",
    "    return df_parcheggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"extract-parkings\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"extract_parkings_dh_core.py\", \"handler\": \"extract_parkings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_parkings = func.run(action=\"job\",inputs={\"di\":data_item_download},outputs={\"parkings\":\"parkings\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Finished at 60 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_parkings,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://parcheggi_not_local/dataitems/table/parkings:713b48b6-e86c-447f-a08e-c02ba00773da'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_parkings = dh.get_dataitem(project=PROJECT_NAME,entity_name=\"parkings\").key\n",
    "data_item_parkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aggregations_parkings_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"aggregations_parkings_dh_core.py\"\n",
    "from datetime import datetime\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"parking_data_aggregated\"])\n",
    "def aggregate_parkings(project, di):\n",
    "    rdf = di.as_df()\n",
    "    rdf['data'] = pd.to_datetime(rdf['data'])\n",
    "    rdf['day'] = rdf['data'].apply(lambda t: t.replace(second=0, minute=0))\n",
    "    rdf['hour'] = rdf['day'].dt.hour\n",
    "    rdf['dow'] = rdf['day'].dt.dayofweek\n",
    "    #rdf['type'] = rdf['data']#.apply(lambda t: \"sadassad\"+t.astype(str))\n",
    "    rdf['day'] = rdf['day'].apply(lambda t: datetime.timestamp(t)) #added because complain of timestamp not JSOn serializable#\n",
    "    rdf = rdf.drop(columns=['data'])\n",
    "    rdf['lat'] = rdf['lat'].apply(lambda t: float(t))\n",
    "    rdf['lon'] = rdf['lon'].apply(lambda t: float(t))\n",
    "    grouped = rdf.groupby(['parcheggio','day']).mean() #\n",
    "    df_aggregated = grouped.reset_index()\n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"aggregate-parkings\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"aggregations_parkings_dh_core.py\", \"handler\": \"aggregate_parkings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data_item = run.outputs()['dataset'].key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_aggregate = func.run(action=\"job\",inputs={\"di\":data_item_download},outputs={\"parking_data_aggregated\":\"parking_data_aggregated\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Finished at 60 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_aggregate,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://parcheggi_not_local/dataitems/table/parking_data_aggregated:5830d558-0bde-4bf4-8488-db7f2e13f8d9'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_aggregate = dh.get_dataitem(project=PROJECT_NAME,entity_name=\"parking_data_aggregated\").key\n",
    "data_item_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_aggregate.outputs()['parking_data_aggregated'].as_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digitalhub_owner_user NznYTgnPsoupfBM7SM6v9cEEpTWFH8if4GoP5TdS2eyFvdZo4gYEUXePCCInjhes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"POSTGRES_USER\"),os.getenv(\"POSTGRES_PASSWORD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parkings_to_db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"parkings_to_db.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import datetime as dtt\n",
    "import os\n",
    "\n",
    "@handler()\n",
    "def to_db(project, agg_di , parkings_di ):\n",
    "    USERNAME = os.getenv(\"POSTGRES_USER\")#project.get_secret(entity_name='DB_USERNAME').read_secret_value()\n",
    "    PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")#project.get_secret(entity_name='DB_PASSWORD').read_secret_value()\n",
    "    engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/digitalhub')\n",
    "    \n",
    "    agg_df = agg_di.as_df(file_format=\"parquet\")\n",
    "        \n",
    "    # Keep only last two calendar years\n",
    "    date = dtt.date.today() - dtt.timedelta(days=365*2)\n",
    "    agg_df['day'] = agg_df['day'].apply(lambda t: datetime.fromtimestamp(t)) #added because before was converted the type\n",
    "    agg_df = agg_df[agg_df['day'].dt.date >= date]\n",
    "    agg_df.to_sql(\"parking_data_aggregated\", engine, if_exists=\"replace\")\n",
    "    parkings_di.as_df().to_sql('parkings', engine, if_exists=\"replace\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"to-db\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         requirements=[\"sqlalchemy\"],\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"parkings_to_db.py\", \"handler\": \"to_db\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set secrets\n",
    "#print(os.getenv(\"POSTGRES_USER\"),os.getenv(\"POSTGRES_PASSWORD\"))\n",
    "#user = os.getenv(\"POSTGRES_USER\")\n",
    "#password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "#secret_a = proj.new_secret(name=\"DB_USERNAME_NEW\", secret_value=user)\n",
    "#secret_b = proj.new_secret(name=\"DB_PASSWORD\", secret_value=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N:B; this might get stuck for some low RAM issues \n",
    "run_to_db = func.run(action=\"job\",inputs={\"agg_di\":data_item_aggregate,\"parkings_di\":data_item_parkings},outputs={})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Finished at 60 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_to_db,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Management Pipeline\n",
    "We create a data management pipeline that executes the data management functions in the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parkings_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"parkings_pipeline.py\"\n",
    "\n",
    "from digitalhub_runtime_kfp.dsl import pipeline_context\n",
    "\n",
    "def myhandler(url):\n",
    "    with pipeline_context() as pc:\n",
    "        s1_dataset = pc.step(name=\"download\", function=\"downloader-funct\", action=\"job\",inputs={\"url\":url},outputs={\"dataset\":\"dataset\"})\n",
    "        #data_item_download = s1_dataset.outputs()[‘dataset’].key\n",
    "        \n",
    "        s2_parking = pc.step(name=\"extract_parking\", function=\"extract-parkings\", action=\"job\",inputs={\"di\":s1_dataset.outputs['dataset']},outputs={\"parkings\":\"parkings\"})\n",
    "        \n",
    "        s3_aggregate = pc.step(name=\"aggregate\",  function=\"aggregate-parkings\", action=\"job\",inputs={\"di\":s1_dataset.outputs['dataset']},outputs={\"parking_data_aggregated\":\"parking_data_aggregated\"})\n",
    "        #data_item_aggregate = s3_aggregate.outputs()[‘parking_data_aggregated’].key\n",
    "        \n",
    "        s4_to_db = pc.step(name=\"to_db\",  function=\"to-db\", action=\"job\",inputs={\"agg_di\": s3_aggregate.outputs['parking_data_aggregated'],\"parkings_di\":s1_dataset.outputs['dataset']},outputs={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = proj.new_workflow(name=\"pipeline_parcheggi\", kind=\"kfp\", source={\"source\": \"parkings_pipeline.py\"}, handler=\"myhandler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n",
    "workflow_run = workflow.run(parameters={\"url\": di.key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "PROJECT_NAME = \"MLparcheggi_non_locale\"\n",
    "ml_proj = dh.get_or_create_project(PROJECT_NAME) # source=\"git://github.com/scc-digitalhub/gdb-project-parkings.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_multimodel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"train_multimodel.py\"\n",
    "\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "from darts import TimeSeries\n",
    "\n",
    "from darts.models import NBEATSModel\n",
    "from darts.metrics import mape, smape, mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pickle import dumps\n",
    "\n",
    "def fill_missing(parc_df):\n",
    "    missing = []  # List to store timestamps for which values could not be filled\n",
    "    temp = pd.Series(parc_df.index.date).value_counts()  # Count the occurrences of each date\n",
    "    temp = temp[temp < 48]  # Filter dates with less than 48 occurrences\n",
    "    temp.sort_index(inplace=True)  # Sort the dates in ascending order\n",
    "    for t in temp.index:  # Iterate through the filtered dates\n",
    "        for h in range(24):  # Iterate through 24 hours\n",
    "            for half_hour in [0, 30]:  # Iterate through 0 and 30 minutes\n",
    "                ts = datetime.datetime(t.year, t.month, t.day, h, half_hour)  # Create a timestamp\n",
    "                if ts not in parc_df.index:  # If the timestamp is missing in the DataFrame\n",
    "                    if ts - datetime.timedelta(days=7) in parc_df.index:  # Check if the previous week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts - datetime.timedelta(days=7)].copy()  # Copy values from the previous week\n",
    "                    elif ts + datetime.timedelta(days=7) in parc_df.index:  # Check if the next week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts + datetime.timedelta(days=7)].copy()  # Copy values from the next week\n",
    "                    else:\n",
    "                        missing.append(ts)  # If values cannot be filled, add the timestamp to the missing list\n",
    "    return missing  # Return the list of timestamps for which values could not be filled\n",
    "\n",
    "\n",
    "@handler()\n",
    "def train_model(project, parkings_di,n_epochs: int = 1, window: int = 60, \n",
    "                input_chunk_length: int = 24, output_chunk_length: int = 12, \n",
    "                split_ratio: float = 0.8):\n",
    "\n",
    "    # Load the input data\n",
    "    df_source = parkings_di.as_df()\n",
    "    # Clean the data\n",
    "    df_clean = df_source.copy()\n",
    "    df_clean.data = pd.to_datetime(df_clean.data, utc=True)\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "    df_clean['date_time_slice'] = df_clean.data.dt.round('30min').dt.tz_convert(None)\n",
    "    df_clean = df_clean[df_clean.date_time_slice >= (datetime.datetime.today() - pd.DateOffset(window))]\n",
    "    df_clean = df_clean[df_clean.date_time_slice <= (datetime.datetime.today() - pd.DateOffset(1))]\n",
    "    df_clean.posti_occupati = df_clean.apply(lambda x: max(0, min(x['posti_totali'], x['posti_occupati'])), axis=1)\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "    df_clean = df_clean.drop(columns=['lat', 'lon', 'data', 'posti_totali', 'posti_liberi', 'posti_occupati'])\n",
    "    parcheggi = df_clean['parcheggio'].unique()\n",
    "\n",
    "    train_sets, val_sets = [], []\n",
    "\n",
    "    # Process data for each parking lot\n",
    "    for parcheggio in parcheggi:\n",
    "        parc_df = df_clean[df_clean['parcheggio'] == parcheggio]\n",
    "        parc_df['hour'] = parc_df.date_time_slice.dt.hour\n",
    "        parc_df['dow'] = parc_df.date_time_slice.dt.dayofweek\n",
    "        parc_df = parc_df.drop(columns=['parcheggio'])\n",
    "        parc_df = parc_df.groupby('date_time_slice').agg({'occupied': 'mean', 'hour': 'first', 'dow': 'first'})\n",
    "        fill_missing(parc_df)\n",
    "        ts = TimeSeries.from_dataframe(parc_df,  value_cols='occupied', freq='30min')\n",
    "        ts_scaled = Scaler().fit_transform(ts)\n",
    "        \n",
    "        split = int(len(ts_scaled) * (1 - split_ratio))\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        train, val = ts_scaled[:-split], ts_scaled[-split:]\n",
    "        train_sets.append(train)\n",
    "        val_sets.append(val)\n",
    "\n",
    "    # Train a multi-model using the NBEATS algorithm\n",
    "    multimodel =  NBEATSModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=output_chunk_length,\n",
    "        n_epochs=n_epochs,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Fit the model to the training sets\n",
    "    multimodel.fit(train_sets)\n",
    "    pred = multimodel.predict(n=output_chunk_length*2, series=train_sets[0][:-output_chunk_length*2])\n",
    "\n",
    "    multimodel.save(\"parcheggi_predictor_model.pt\")\n",
    "    with ZipFile(\"parcheggi_predictor_model.pt.zip\", \"w\") as z:\n",
    "        z.write(\"parcheggi_predictor_model.pt\")\n",
    "        z.write(\"parcheggi_predictor_model.pt.ckpt\")\n",
    "    project.log_model(name=\"modello_parcheggi\", kind=\"model\", src_path=\"parcheggi_predictor_model.pt.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"training_model\"\n",
    "func = ml_proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         requirements =[\"darts==0.25.0\", \"pandas==1.4.4\", \"numpy==1.22.4\", \"patsy==0.5.2\", \"scikit-learn==1.1.2\"],\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"train_multimodel.py\", \"handler\": \"train_model\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download = 'store://parcheggi/dataitems/table/dataset:a2b7895c-090e-47e3-b721-1628aaa4dbf2'#TODO remove the  = \"store\" this is to bypass part above # relative to the parkings above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train_model = func.run(action=\"job\",inputs={\"parkings_di\":data_item_download},outputs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_build_model = func.run(action=\"build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 0 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_train_model,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'ERROR'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train_model.refresh()\n",
    "run_train_model.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serve_multimodel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"serve_multimodel.py\"\n",
    "\n",
    "from darts.models import NBEATSModel\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def init_function(context):\n",
    "\n",
    "    # Qua ti setti l'id del modello che vuoi caricare\n",
    "    model_id = \"8be239dc-8798-48ff-bc3b-80d1d80cc2af\"\n",
    "\n",
    "    # prendi l'entity model sulla base dell'id\n",
    "    model = context.project.get_model(entity_id=model_id)\n",
    "    path = model.download()\n",
    "    local_path_model = \"extracted_model/\"\n",
    "    # Qua fai unzip immagino\n",
    "    with ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(local_path_model)\n",
    "    \n",
    "    # codice che carica il modello\n",
    "    input_chunk_length = 24\n",
    "    output_chunk_length = 12\n",
    "    name_model_local = local_path_model +\"parcheggi_predictor_model.pt\"\n",
    "    mm = NBEATSModel(\n",
    "            input_chunk_length,\n",
    "            output_chunk_length\n",
    "    ).load(name_model_local)\n",
    "\n",
    "    # settare model nel context di nuclio (non su project che è il context nostro)\n",
    "    context.setattr(\"model\", mm)\n",
    "\n",
    "def serve(context, event):\n",
    "\n",
    "    # Sostanzialmente invochiamo la funzione con una chiamata REST\n",
    "    # Nel body della richiesta mandi l'inference input\n",
    "    \n",
    "    if isinstance(event.body, bytes):\n",
    "        body = json.loads(event.body)\n",
    "    context.logger.info(f\"Received event: {body}\")\n",
    "    inference_input = body[\"inference_input\"]\n",
    "    \n",
    "    return context.model.predict(n=output_chunk_length*2,\n",
    "                                 series=inference_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"serve_model\"\n",
    "func = ml_proj.new_function(name=FUNCTION_NAME,\n",
    "                            kind=\"python\",\n",
    "                            python_version=\"PYTHON3_9\",\n",
    "                            source={\n",
    "                                 \"source\": \"serve_multimodel.py\",\n",
    "                                 \"handler\": \"serve\",\n",
    "                                 \"init_function\": \"init\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_serve_model = func.run(action=\"serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0, status: READY ...\n",
      "Finished at 10 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_serve_model,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitalhub-core",
   "language": "python",
   "name": "digitalhub-core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
