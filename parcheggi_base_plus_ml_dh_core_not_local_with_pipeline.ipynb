{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "### 1.1. Download data\n",
    "Download data from the API, and load it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/exports/csv?lang=it&timezone=UTC&use_labels=true&delimiter=%3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Platform Support - Data Ops\n",
    "\n",
    "We use the platform support to load the data into the platform, version it, and automate the execution of the data management operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initalization\n",
    "Create the working context: data management project for the parking data processing. Project is a placeholder for the code, data, and management of the parking data operations. To keep it reproducible, we use the `git` source type to store the definition and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_funtion_complete(rrun,seconds = 1,status =False):\n",
    "    import time\n",
    "    rrun.refresh()\n",
    "    state = rrun.status\n",
    "    #print(state)\n",
    "    v = 0\n",
    "    while state.state =='READY' or state.state =='RUNNING':\n",
    "        print(f\"Seconds: {v}, {'status: '+ state.state if status else ''} ...\")\n",
    "        v+=seconds\n",
    "        rrun.refresh()\n",
    "        state = rrun.status\n",
    "        time.sleep(seconds)\n",
    "    print(f\"Finished at {v} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "PROJECT_NAME = \"MLparksrem\"\n",
    "proj = dh.get_or_create_project(PROJECT_NAME) # source=\"git://github.com/scc-digitalhub/gdb-project-parkings.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data management functions\n",
    "We convert the data management ETL operations into functions - single executable operations that can be executed in the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing download_all_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"download_all_dh_core.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"dataset\"])\n",
    "def downloader(project, url):\n",
    "    df = url.as_df(file_format='csv',sep=\";\")\n",
    "    df[['lat', 'lon']] = df['coordinate'].str.split(', ',expand=True)\n",
    "    df = df.drop(columns=['% occupazione', 'GUID', 'coordinate']).rename(columns={'Parcheggio': 'parcheggio', 'Data': 'data', 'Posti liberi': 'posti_liberi', 'Posti occupati': 'posti_occupati', 'Posti totali': 'posti_totali'})\n",
    "    df[\"lat\"] = pd.to_numeric(df[\"lat\"])\n",
    "    df[\"lon\"] = pd.to_numeric(df[\"lon\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"downloader-funct\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"download_all_dh_core.py\", \"handler\": \"downloader\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download = func.run(action=\"job\",inputs={\"url\":di.key},outputs={\"dataset\":\"dataset\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Seconds: 60,  ...\n",
      "Seconds: 70,  ...\n",
      "Finished at 80 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_download,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_download.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://MLparksrem/dataitems/table/dataset:3d2b1181-a7f2-4e2a-8a14-842dcc49f17e'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_download  = dh.get_dataitem(project=PROJECT_NAME,entity_name=\"dataset\").key\n",
    "data_item_download  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_item_download = proj.new_dataitem(name=\"dataset\", kind=\"table\", path=\"s3://datalake/parcheggi/dataitems/f2024e9f-6dda-4a77-9216-80713b881300/data.parquet\")#run_download.outputs()['dataset'].key\n",
    "#data_item_download = data_item_download.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing extract_parkings_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"extract_parkings_dh_core.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"parkings\"])\n",
    "def extract_parkings(project, di):\n",
    "    KEYS = ['parcheggio', 'lat', 'lon', 'posti_totali']\n",
    "    df_parcheggi = di.as_df().groupby(['parcheggio']).first().reset_index()[KEYS]\n",
    "    return df_parcheggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"extract-parkings\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"extract_parkings_dh_core.py\", \"handler\": \"extract_parkings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_parkings = func.run(action=\"job\",inputs={\"di\":data_item_download},outputs={\"parkings\":\"parkings\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Seconds: 60,  ...\n",
      "Seconds: 70,  ...\n",
      "Seconds: 80,  ...\n",
      "Seconds: 90,  ...\n",
      "Seconds: 100,  ...\n",
      "Seconds: 110,  ...\n",
      "Finished at 120 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_parkings,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://MLparksrem/dataitems/table/parkings:b53e00c8-d707-4d3e-abe2-2d8f82e301bc'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_parkings = dh.get_dataitem(project=PROJECT_NAME,entity_name=\"parkings\").key\n",
    "data_item_parkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aggregations_parkings_dh_core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"aggregations_parkings_dh_core.py\"\n",
    "from datetime import datetime\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"parking_data_aggregated\"])\n",
    "def aggregate_parkings(project, di):\n",
    "    rdf = di.as_df()\n",
    "    rdf['data'] = pd.to_datetime(rdf['data'])\n",
    "    rdf['day'] = rdf['data'].apply(lambda t: t.replace(second=0, minute=0))\n",
    "    rdf['hour'] = rdf['day'].dt.hour\n",
    "    rdf['dow'] = rdf['day'].dt.dayofweek\n",
    "    #rdf['type'] = rdf['data']#.apply(lambda t: \"sadassad\"+t.astype(str))\n",
    "    rdf['day'] = rdf['day'].apply(lambda t: datetime.timestamp(t)) #added because complain of timestamp not JSOn serializable#\n",
    "    rdf = rdf.drop(columns=['data'])\n",
    "    rdf['lat'] = rdf['lat'].apply(lambda t: float(t))\n",
    "    rdf['lon'] = rdf['lon'].apply(lambda t: float(t))\n",
    "    grouped = rdf.groupby(['parcheggio','day']).mean() #\n",
    "    df_aggregated = grouped.reset_index()\n",
    "    return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"aggregate-parkings\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"aggregations_parkings_dh_core.py\", \"handler\": \"aggregate_parkings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data_item = run.outputs()['dataset'].key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_aggregate = func.run(action=\"job\",inputs={\"di\":data_item_download},outputs={\"parking_data_aggregated\":\"parking_data_aggregated\"})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Finished at 60 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_aggregate,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://MLparksrem/dataitems/table/parking_data_aggregated:79f86ea6-31d7-46a7-b95e-0f75a697cb70'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_aggregate = dh.get_dataitem(project=PROJECT_NAME,entity_name=\"parking_data_aggregated\").key\n",
    "data_item_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_aggregate.outputs()['parking_data_aggregated'].as_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digitalhub_owner_user 2M7ogUQieEcyDOjldtc90QbxnUvJmOnknQCRBlZ1Mw1xFJWEBpQcudjxx5sQ1f7h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"POSTGRES_USER\"),os.getenv(\"POSTGRES_PASSWORD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parkings_to_db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"parkings_to_db.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import datetime as dtt\n",
    "import os\n",
    "\n",
    "@handler()\n",
    "def to_db(project, agg_di , parkings_di ):\n",
    "    USERNAME = os.getenv(\"POSTGRES_USER\")#project.get_secret(entity_name='DB_USERNAME').read_secret_value()\n",
    "    PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")#project.get_secret(entity_name='DB_PASSWORD').read_secret_value()\n",
    "    engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/digitalhub')\n",
    "    \n",
    "    agg_df = agg_di.as_df(file_format=\"parquet\")\n",
    "        \n",
    "    # Keep only last two calendar years\n",
    "    date = dtt.date.today() - dtt.timedelta(days=365*2)\n",
    "    agg_df['day'] = agg_df['day'].apply(lambda t: datetime.fromtimestamp(t)) #added because before was converted the type\n",
    "    agg_df = agg_df[agg_df['day'].dt.date >= date]\n",
    "    agg_df.to_sql(\"parking_data_aggregated\", engine, if_exists=\"replace\")\n",
    "    parkings_di.as_df().to_sql('parkings', engine, if_exists=\"replace\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"to-db\"\n",
    "func = proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         requirements=[\"sqlalchemy\"],\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"parkings_to_db.py\", \"handler\": \"to_db\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set secrets\n",
    "#print(os.getenv(\"POSTGRES_USER\"),os.getenv(\"POSTGRES_PASSWORD\"))\n",
    "#user = os.getenv(\"POSTGRES_USER\")\n",
    "#password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "#secret_a = proj.new_secret(name=\"DB_USERNAME_NEW\", secret_value=user)\n",
    "#secret_b = proj.new_secret(name=\"DB_PASSWORD\", secret_value=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N:B; this might get stuck for some low RAM issues \n",
    "run_to_db = func.run(action=\"job\",inputs={\"agg_di\":data_item_aggregate,\"parkings_di\":data_item_parkings},outputs={})# local_execution=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0,  ...\n",
      "Seconds: 10,  ...\n",
      "Seconds: 20,  ...\n",
      "Seconds: 30,  ...\n",
      "Seconds: 40,  ...\n",
      "Seconds: 50,  ...\n",
      "Finished at 60 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_to_db,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Management Pipeline\n",
    "We create a data management pipeline that executes the data management functions in the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parkings_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"parkings_pipeline.py\"\n",
    "\n",
    "from digitalhub_runtime_kfp.dsl import pipeline_context\n",
    "\n",
    "def myhandler(url):\n",
    "    with pipeline_context() as pc:\n",
    "        s1_dataset = pc.step(name=\"download\", function=\"downloader-funct\", action=\"job\",inputs={\"url\":url},outputs={\"dataset\":\"dataset\"})\n",
    "        #data_item_download = s1_dataset.outputs()[‘dataset’].key\n",
    "        \n",
    "        s2_parking = pc.step(name=\"extract_parking\", function=\"extract-parkings\", action=\"job\",inputs={\"di\":s1_dataset.outputs['dataset']},outputs={\"parkings\":\"parkings\"})\n",
    "        \n",
    "        s3_aggregate = pc.step(name=\"aggregate\",  function=\"aggregate-parkings\", action=\"job\",inputs={\"di\":s1_dataset.outputs['dataset']},outputs={\"parking_data_aggregated\":\"parking_data_aggregated\"})\n",
    "        #data_item_aggregate = s3_aggregate.outputs()[‘parking_data_aggregated’].key\n",
    "        \n",
    "        s4_to_db = pc.step(name=\"to_db\",  function=\"to-db\", action=\"job\",inputs={\"agg_di\": s3_aggregate.outputs['parking_data_aggregated'],\"parkings_di\":s1_dataset.outputs['dataset']},outputs={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = proj.new_workflow(name=\"pipeline_parcheggi\", kind=\"kfp\", source={\"source\": \"parkings_pipeline.py\"}, handler=\"myhandler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n",
    "workflow_run = workflow.run(parameters={\"url\": di.key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "\n",
    "PROJECT_NAME = \"MLparksrem\"\n",
    "ml_proj = dh.get_or_create_project(PROJECT_NAME) # source=\"git://github.com/scc-digitalhub/gdb-project-parkings.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_multimodel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"train_multimodel.py\"\n",
    "\n",
    "import pandas as pd\n",
    "from digitalhub_runtime_python import handler\n",
    "from darts import TimeSeries\n",
    "\n",
    "from darts.models import NBEATSModel\n",
    "from darts.metrics import mape, smape, mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pickle import dumps\n",
    "\n",
    "def fill_missing(parc_df):\n",
    "    missing = []  # List to store timestamps for which values could not be filled\n",
    "    temp = pd.Series(parc_df.index.date).value_counts()  # Count the occurrences of each date\n",
    "    temp = temp[temp < 48]  # Filter dates with less than 48 occurrences\n",
    "    temp.sort_index(inplace=True)  # Sort the dates in ascending order\n",
    "    for t in temp.index:  # Iterate through the filtered dates\n",
    "        for h in range(24):  # Iterate through 24 hours\n",
    "            for half_hour in [0, 30]:  # Iterate through 0 and 30 minutes\n",
    "                ts = datetime.datetime(t.year, t.month, t.day, h, half_hour)  # Create a timestamp\n",
    "                if ts not in parc_df.index:  # If the timestamp is missing in the DataFrame\n",
    "                    if ts - datetime.timedelta(days=7) in parc_df.index:  # Check if the previous week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts - datetime.timedelta(days=7)].copy()  # Copy values from the previous week\n",
    "                    elif ts + datetime.timedelta(days=7) in parc_df.index:  # Check if the next week's timestamp is available\n",
    "                        parc_df.loc[ts] = parc_df.loc[ts + datetime.timedelta(days=7)].copy()  # Copy values from the next week\n",
    "                    else:\n",
    "                        missing.append(ts)  # If values cannot be filled, add the timestamp to the missing list\n",
    "    return missing  # Return the list of timestamps for which values could not be filled\n",
    "\n",
    "\n",
    "@handler()\n",
    "def train_model(project, parkings_di,n_epochs: int = 1, window: int = 60, \n",
    "                input_chunk_length: int = 24, output_chunk_length: int = 12, \n",
    "                split_ratio: float = 0.8):\n",
    "\n",
    "    # Load the input data\n",
    "    df_source = parkings_di.as_df()\n",
    "    # Clean the data\n",
    "    df_clean = df_source.copy()\n",
    "    df_clean.data = pd.to_datetime(df_clean.data, utc=True)\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "    df_clean['date_time_slice'] = df_clean.data.dt.round('30min').dt.tz_convert(None)\n",
    "    df_clean = df_clean[df_clean.date_time_slice >= (datetime.datetime.today() - pd.DateOffset(window))]\n",
    "    df_clean = df_clean[df_clean.date_time_slice <= (datetime.datetime.today() - pd.DateOffset(1))]\n",
    "    df_clean.posti_occupati = df_clean.apply(lambda x: max(0, min(x['posti_totali'], x['posti_occupati'])), axis=1)\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "    df_clean = df_clean.drop(columns=['lat', 'lon', 'data', 'posti_totali', 'posti_liberi', 'posti_occupati'])\n",
    "    parcheggi = df_clean['parcheggio'].unique()\n",
    "\n",
    "    train_sets, val_sets = [], []\n",
    "\n",
    "    # Process data for each parking lot\n",
    "    for parcheggio in parcheggi:\n",
    "        parc_df = df_clean[df_clean['parcheggio'] == parcheggio]\n",
    "        parc_df['hour'] = parc_df.date_time_slice.dt.hour\n",
    "        parc_df['dow'] = parc_df.date_time_slice.dt.dayofweek\n",
    "        parc_df = parc_df.drop(columns=['parcheggio'])\n",
    "        parc_df = parc_df.groupby('date_time_slice').agg({'occupied': 'mean', 'hour': 'first', 'dow': 'first'})\n",
    "        fill_missing(parc_df)\n",
    "        ts = TimeSeries.from_dataframe(parc_df,  value_cols='occupied', freq='30min')\n",
    "        ts_scaled = Scaler().fit_transform(ts)\n",
    "        \n",
    "        split = int(len(ts_scaled) * (1 - split_ratio))\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        train, val = ts_scaled[:-split], ts_scaled[-split:]\n",
    "        train_sets.append(train)\n",
    "        val_sets.append(val)\n",
    "\n",
    "    # Train a multi-model using the NBEATS algorithm\n",
    "    multimodel =  NBEATSModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=output_chunk_length,\n",
    "        n_epochs=n_epochs,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Fit the model to the training sets\n",
    "    multimodel.fit(train_sets)\n",
    "    pred = multimodel.predict(n=output_chunk_length*2, series=train_sets[0][:-output_chunk_length*2])\n",
    "\n",
    "    multimodel.save(\"parcheggi_predictor_model.pt\")\n",
    "    with ZipFile(\"parcheggi_predictor_model.pt.zip\", \"w\") as z:\n",
    "        z.write(\"parcheggi_predictor_model.pt\")\n",
    "        z.write(\"parcheggi_predictor_model.pt.ckpt\")\n",
    "    project.log_model(name=\"modelloparcheggi\", kind=\"model\", source_path=\"parcheggi_predictor_model.pt.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"training_model\"\n",
    "func = ml_proj.new_function(name=FUNCTION_NAME,\n",
    "                         kind=\"python\",\n",
    "                         #requirements =[\"darts==0.25.0\", \"pandas==1.4.4\", \"numpy==1.22.4\", \"patsy==0.5.2\", \"scikit-learn==1.1.2\"], # darts was heavy in GB so install torch with no cuda in the run build\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         base_image = \"python:3.9\",\n",
    "                         source={\"source\": \"train_multimodel.py\", \"handler\": \"train_model\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://MLparksrem/dataitems/table/dataset:3d2b1181-a7f2-4e2a-8a14-842dcc49f17e'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_download # relative to the parkings above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_build_model = func.run(action=\"build\", instructions=[\"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\",\"pip3 install darts==0.25.0 pandas==1.4.4 numpy==1.22.4 patsy==0.5.2 scikit-learn==1.1.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0, status: RUNNING ...\n",
      "Seconds: 10, status: RUNNING ...\n",
      "Seconds: 20, status: RUNNING ...\n",
      "Seconds: 30, status: RUNNING ...\n",
      "Seconds: 40, status: RUNNING ...\n",
      "Seconds: 50, status: RUNNING ...\n",
      "Seconds: 60, status: RUNNING ...\n",
      "Seconds: 70, status: RUNNING ...\n",
      "Seconds: 80, status: RUNNING ...\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_build_model,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train_model = func.run(action=\"job\",inputs={\"parkings_di\":data_item_download},outputs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0, status: RUNNING ...\n",
      "Seconds: 10, status: RUNNING ...\n",
      "Seconds: 20, status: RUNNING ...\n",
      "Seconds: 30, status: RUNNING ...\n",
      "Seconds: 40, status: RUNNING ...\n"
     ]
    },
    {
     "ename": "BackendError",
     "evalue": "Backend error. Status code: 404. Reason: {\"status\":404,\"message\":\"No such EntityName.RUN(value=run).\",\"errorCode\":null}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/digitalhub-core/lib/python3.9/site-packages/digitalhub_core/client/objects/dhcore.py:212\u001b[0m, in \u001b[0;36mClientDHCore._call\u001b[0;34m(self, call_type, api, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m response \u001b[38;5;241m=\u001b[39m request(call_type, url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 212\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Api-Level\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders:\n",
      "File \u001b[0;32m/opt/conda/envs/digitalhub-core/lib/python3.9/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error:  for url: http://digitalhub-tenant1-core:8080/api/v1/-/MLparksrem/runs/df00422b-6cc9-496d-91a6-d3897bcf25fd",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrunning_funtion_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_train_model\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mrunning_funtion_complete\u001b[0;34m(rrun, seconds, status)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeconds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mstate\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mstatus\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m v\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mseconds\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m state \u001b[38;5;241m=\u001b[39m rrun\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m     12\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(seconds)\n",
      "File \u001b[0;32m/opt/conda/envs/digitalhub-core/lib/python3.9/site-packages/digitalhub_core/entities/runs/entity.py:291\u001b[0m, in \u001b[0;36mRun.refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03mRefresh object from backend.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    Run object.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m api \u001b[38;5;241m=\u001b[39m api_ctx_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENTITY_TYPE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 291\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_attributes(obj)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/digitalhub-core/lib/python3.9/site-packages/digitalhub_core/context/context.py:64\u001b[0m, in \u001b[0;36mContext.read_object\u001b[0;34m(self, api, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_object\u001b[39m(\u001b[38;5;28mself\u001b[39m, api: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Read an object.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m        Response object.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/digitalhub-core/lib/python3.9/site-packages/digitalhub_core/client/objects/dhcore.py:101\u001b[0m, in \u001b[0;36mClientDHCore.read_object\u001b[0;34m(self, api, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_object\u001b[39m(\u001b[38;5;28mself\u001b[39m, api: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Get an object from DHCore.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m        Response object.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/digitalhub-core/lib/python3.9/site-packages/digitalhub_core/client/objects/dhcore.py:229\u001b[0m, in \u001b[0;36mClientDHCore._call\u001b[0;34m(self, call_type, api, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackend error. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Reason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    231\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBackendError\u001b[0m: Backend error. Status code: 404. Reason: {\"status\":404,\"message\":\"No such EntityName.RUN(value=run).\",\"errorCode\":null}"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_train_model,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'ERROR', 'k8s': {'pods': [{'metadata': {'creationTimestamp': 1720792778.0, 'generateName': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e-', 'labels': {'app.kubernetes.io/instance': 'dhcore-5f55c57e-22ca-4464-9dee-7dded732983e', 'app.kubernetes.io/managed-by': 'dhcore', 'app.kubernetes.io/part-of': 'dhcore-mlparksrem', 'app.kubernetes.io/version': '5f55c57e-22ca-4464-9dee-7dded732983e', 'batch.kubernetes.io/controller-uid': '5da2af94-91bf-4798-b2f9-dc33f7980283', 'batch.kubernetes.io/job-name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e', 'controller-uid': '5da2af94-91bf-4798-b2f9-dc33f7980283', 'job-name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}, 'managedFields': [{'apiVersion': 'v1', 'fieldsType': 'FieldsV1', 'manager': 'kube-controller-manager', 'operation': 'Update', 'time': 1720792778.0}, {'apiVersion': 'v1', 'fieldsType': 'FieldsV1', 'manager': 'kubelet', 'operation': 'Update', 'subresource': 'status', 'time': 1720792956.0}], 'name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e-4ctj9', 'namespace': 'digitalhub-tenant1', 'ownerReferences': [{'apiVersion': 'batch/v1', 'blockOwnerDeletion': True, 'controller': True, 'kind': 'Job', 'name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e', 'uid': '5da2af94-91bf-4798-b2f9-dc33f7980283'}], 'resourceVersion': '347040', 'uid': '19b36634-36c3-48b1-8419-51136429e4fe'}, 'spec': {'containers': [{'args': ['--config', '/shared/function.yaml'], 'command': ['/usr/local/bin/processor'], 'env': [{'name': 'DH_RUN_SECRET_NAME', 'value': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'PYTHONPATH', 'value': '${PYTHONPATH}:/shared/'}, {'name': 'DIGITALHUB_CORE_TOKEN', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_TOKEN', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'DIGITALHUB_CORE_AUTH_SUB', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_AUTH_SUB', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'PROJECT_NAME', 'value': 'MLparksrem'}, {'name': 'RUN_ID', 'value': '5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'DIGITALHUB_CORE_USER', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_USER', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}], 'envFrom': [{'configMapRef': {'name': 'digitalhub-common-env'}}, {'secretRef': {'name': 'digitalhub-common-creds'}}], 'image': 'ghcr.io/scc-digitalhub/digitalhub-serverless/python-runtime:3.9', 'imagePullPolicy': 'IfNotPresent', 'name': 'c-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e', 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'volumeMounts': [{'mountPath': '/shared', 'name': 'shared-dir'}, {'mountPath': '/init-config-map', 'name': 'init-config-map'}, {'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount', 'name': 'kube-api-access-ncnvt', 'readOnly': True}]}], 'dnsPolicy': 'ClusterFirst', 'enableServiceLinks': True, 'imagePullSecrets': [{'name': 'registry-credentials'}], 'initContainers': [{'command': ['/bin/bash', '-c', '/app/builder-tool.sh'], 'env': [{'name': 'DH_RUN_SECRET_NAME', 'value': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'PYTHONPATH', 'value': '${PYTHONPATH}:/shared/'}, {'name': 'DIGITALHUB_CORE_TOKEN', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_TOKEN', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'DIGITALHUB_CORE_AUTH_SUB', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_AUTH_SUB', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'PROJECT_NAME', 'value': 'MLparksrem'}, {'name': 'RUN_ID', 'value': '5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'DIGITALHUB_CORE_USER', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_USER', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}], 'envFrom': [{'configMapRef': {'name': 'digitalhub-common-env'}}, {'secretRef': {'name': 'digitalhub-common-creds'}}], 'image': 'ghcr.io/scc-digitalhub/digitalhub-core-builder-tool:latest', 'imagePullPolicy': 'Always', 'name': 'init-container-5f55c57e-22ca-4464-9dee-7dded732983e', 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'volumeMounts': [{'mountPath': '/shared', 'name': 'shared-dir'}, {'mountPath': '/init-config-map', 'name': 'init-config-map'}, {'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount', 'name': 'kube-api-access-ncnvt', 'readOnly': True}]}], 'nodeName': 'digitalhub-dev-md-0-bhd5r-w788w', 'preemptionPolicy': 'PreemptLowerPriority', 'priority': 0, 'restartPolicy': 'Never', 'schedulerName': 'default-scheduler', 'serviceAccount': 'default', 'serviceAccountName': 'default', 'terminationGracePeriodSeconds': 30, 'tolerations': [{'effect': 'NoExecute', 'key': 'node.kubernetes.io/not-ready', 'operator': 'Exists', 'tolerationSeconds': 300}, {'effect': 'NoExecute', 'key': 'node.kubernetes.io/unreachable', 'operator': 'Exists', 'tolerationSeconds': 300}], 'volumes': [{'emptyDir': {'sizeLimit': {'number': 104857600, 'format': 'BINARY_SI'}}, 'name': 'shared-dir'}, {'configMap': {'defaultMode': 420, 'name': 'init-config-map-5f55c57e-22ca-4464-9dee-7dded732983e'}, 'name': 'init-config-map'}, {'name': 'kube-api-access-ncnvt', 'projected': {'defaultMode': 420, 'sources': [{'serviceAccountToken': {'expirationSeconds': 3607, 'path': 'token'}}, {'configMap': {'items': [{'key': 'ca.crt', 'path': 'ca.crt'}], 'name': 'kube-root-ca.crt'}}, {'downwardAPI': {'items': [{'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.namespace'}, 'path': 'namespace'}]}}]}}]}, 'status': {'conditions': [{'lastTransitionTime': 1720792955.0, 'status': 'False', 'type': 'PodReadyToStartContainers'}, {'lastTransitionTime': 1720792780.0, 'status': 'True', 'type': 'Initialized'}, {'lastTransitionTime': 1720792954.0, 'reason': 'PodFailed', 'status': 'False', 'type': 'Ready'}, {'lastTransitionTime': 1720792954.0, 'reason': 'PodFailed', 'status': 'False', 'type': 'ContainersReady'}, {'lastTransitionTime': 1720792778.0, 'status': 'True', 'type': 'PodScheduled'}], 'containerStatuses': [{'containerID': 'containerd://3089f2245695ccc057bfd24baf34d39be9b51bdf59410626dc913265da07ccd7', 'image': 'ghcr.io/scc-digitalhub/digitalhub-serverless/python-runtime:3.9', 'imageID': 'ghcr.io/scc-digitalhub/digitalhub-serverless/python-runtime@sha256:a71a9e521157a087af79645936137d183122ddedc72e67aad95a9a2bbe54d2f4', 'name': 'c-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e', 'ready': False, 'restartCount': 0, 'started': False, 'state': {'terminated': {'containerID': 'containerd://3089f2245695ccc057bfd24baf34d39be9b51bdf59410626dc913265da07ccd7', 'exitCode': 1, 'finishedAt': 1720792953.0, 'reason': 'Error', 'startedAt': 1720792781.0}}}], 'hostIP': '10.1.0.5', 'hostIPs': [{'ip': '10.1.0.5'}], 'initContainerStatuses': [{'containerID': 'containerd://94484d0eeaf3a4016bdd4f9d03bf5ac4b1da7000c7692a23d42ea6f046fd553d', 'image': 'ghcr.io/scc-digitalhub/digitalhub-core-builder-tool:latest', 'imageID': 'ghcr.io/scc-digitalhub/digitalhub-core-builder-tool@sha256:bbc06eee8fc253863d0cd10286e1914c688988588838a45a4ca732222765e72e', 'name': 'init-container-5f55c57e-22ca-4464-9dee-7dded732983e', 'ready': True, 'restartCount': 0, 'started': False, 'state': {'terminated': {'containerID': 'containerd://94484d0eeaf3a4016bdd4f9d03bf5ac4b1da7000c7692a23d42ea6f046fd553d', 'exitCode': 0, 'finishedAt': 1720792779.0, 'reason': 'Completed', 'startedAt': 1720792779.0}}}], 'phase': 'Failed', 'podIP': '192.168.2.235', 'podIPs': [{'ip': '192.168.2.235'}], 'qosClass': 'BestEffort', 'startTime': 1720792778.0}}], 'job': {'metadata': {'creationTimestamp': 1720792778.0, 'generation': 1, 'labels': {'app.kubernetes.io/instance': 'dhcore-5f55c57e-22ca-4464-9dee-7dded732983e', 'app.kubernetes.io/managed-by': 'dhcore', 'app.kubernetes.io/part-of': 'dhcore-mlparksrem', 'app.kubernetes.io/version': '5f55c57e-22ca-4464-9dee-7dded732983e'}, 'managedFields': [{'apiVersion': 'batch/v1', 'fieldsType': 'FieldsV1', 'manager': 'Kubernetes Java Client', 'operation': 'Update', 'time': 1720792778.0}, {'apiVersion': 'batch/v1', 'fieldsType': 'FieldsV1', 'manager': 'kube-controller-manager', 'operation': 'Update', 'subresource': 'status', 'time': 1720792956.0}], 'name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e', 'namespace': 'digitalhub-tenant1', 'resourceVersion': '347041', 'uid': '5da2af94-91bf-4798-b2f9-dc33f7980283'}, 'apiVersion': 'batch/v1', 'kind': 'Job', 'spec': {'activeDeadlineSeconds': 259200, 'backoffLimit': 0, 'completionMode': 'NonIndexed', 'completions': 1, 'manualSelector': False, 'parallelism': 1, 'podReplacementPolicy': 'TerminatingOrFailed', 'selector': {'matchLabels': {'batch.kubernetes.io/controller-uid': '5da2af94-91bf-4798-b2f9-dc33f7980283'}}, 'suspend': False, 'template': {'metadata': {'labels': {'app.kubernetes.io/instance': 'dhcore-5f55c57e-22ca-4464-9dee-7dded732983e', 'app.kubernetes.io/managed-by': 'dhcore', 'app.kubernetes.io/part-of': 'dhcore-mlparksrem', 'app.kubernetes.io/version': '5f55c57e-22ca-4464-9dee-7dded732983e', 'batch.kubernetes.io/controller-uid': '5da2af94-91bf-4798-b2f9-dc33f7980283', 'batch.kubernetes.io/job-name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e', 'controller-uid': '5da2af94-91bf-4798-b2f9-dc33f7980283', 'job-name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}, 'name': 'j-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}, 'spec': {'containers': [{'args': ['--config', '/shared/function.yaml'], 'command': ['/usr/local/bin/processor'], 'env': [{'name': 'DH_RUN_SECRET_NAME', 'value': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'PYTHONPATH', 'value': '${PYTHONPATH}:/shared/'}, {'name': 'DIGITALHUB_CORE_TOKEN', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_TOKEN', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'DIGITALHUB_CORE_AUTH_SUB', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_AUTH_SUB', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'PROJECT_NAME', 'value': 'MLparksrem'}, {'name': 'RUN_ID', 'value': '5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'DIGITALHUB_CORE_USER', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_USER', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}], 'envFrom': [{'configMapRef': {'name': 'digitalhub-common-env'}}, {'secretRef': {'name': 'digitalhub-common-creds'}}], 'image': 'ghcr.io/scc-digitalhub/digitalhub-serverless/python-runtime:3.9', 'imagePullPolicy': 'IfNotPresent', 'name': 'c-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e', 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'volumeMounts': [{'mountPath': '/shared', 'name': 'shared-dir'}, {'mountPath': '/init-config-map', 'name': 'init-config-map'}]}], 'dnsPolicy': 'ClusterFirst', 'imagePullSecrets': [{'name': 'registry-credentials'}], 'initContainers': [{'command': ['/bin/bash', '-c', '/app/builder-tool.sh'], 'env': [{'name': 'DH_RUN_SECRET_NAME', 'value': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'PYTHONPATH', 'value': '${PYTHONPATH}:/shared/'}, {'name': 'DIGITALHUB_CORE_TOKEN', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_TOKEN', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'DIGITALHUB_CORE_AUTH_SUB', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_AUTH_SUB', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}, {'name': 'PROJECT_NAME', 'value': 'MLparksrem'}, {'name': 'RUN_ID', 'value': '5f55c57e-22ca-4464-9dee-7dded732983e'}, {'name': 'DIGITALHUB_CORE_USER', 'valueFrom': {'secretKeyRef': {'key': 'DIGITALHUB_CORE_USER', 'name': 'sec-python-python-job-5f55c57e-22ca-4464-9dee-7dded732983e'}}}], 'envFrom': [{'configMapRef': {'name': 'digitalhub-common-env'}}, {'secretRef': {'name': 'digitalhub-common-creds'}}], 'image': 'ghcr.io/scc-digitalhub/digitalhub-core-builder-tool:latest', 'imagePullPolicy': 'Always', 'name': 'init-container-5f55c57e-22ca-4464-9dee-7dded732983e', 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'volumeMounts': [{'mountPath': '/shared', 'name': 'shared-dir'}, {'mountPath': '/init-config-map', 'name': 'init-config-map'}]}], 'restartPolicy': 'Never', 'schedulerName': 'default-scheduler', 'terminationGracePeriodSeconds': 30, 'volumes': [{'emptyDir': {'sizeLimit': {'number': 104857600, 'format': 'BINARY_SI'}}, 'name': 'shared-dir'}, {'configMap': {'defaultMode': 420, 'name': 'init-config-map-5f55c57e-22ca-4464-9dee-7dded732983e'}, 'name': 'init-config-map'}]}}}, 'status': {'conditions': [{'lastProbeTime': 1720792956.0, 'lastTransitionTime': 1720792956.0, 'message': 'Job has reached the specified backoff limit', 'reason': 'BackoffLimitExceeded', 'status': 'True', 'type': 'Failed'}], 'failed': 1, 'ready': 0, 'startTime': 1720792778.0, 'terminating': 0}}}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train_model.refresh()\n",
    "run_train_model.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serve_multimodel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"serve_multimodel.py\"\n",
    "\n",
    "from darts.models import NBEATSModel\n",
    "from zipfile import ZipFile\n",
    "\n",
    "#def init_function(context):\n",
    "def init(context):\n",
    "    # Qua ti setti l'id del modello che vuoi caricare\n",
    "    model_id = \"8be239dc-8798-48ff-bc3b-80d1d80cc2af\"\n",
    "\n",
    "    # prendi l'entity model sulla base dell'id\n",
    "    model = context.project.get_model(entity_id=model_id)\n",
    "    path = model.download()\n",
    "    local_path_model = \"extracted_model/\"\n",
    "    # Qua fai unzip immagino\n",
    "    with ZipFile(path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(local_path_model)\n",
    "    \n",
    "    # codice che carica il modello\n",
    "    input_chunk_length = 24\n",
    "    output_chunk_length = 12\n",
    "    name_model_local = local_path_model +\"parcheggi_predictor_model.pt\"\n",
    "    mm = NBEATSModel(\n",
    "            input_chunk_length,\n",
    "            output_chunk_length\n",
    "    ).load(name_model_local)\n",
    "\n",
    "    # settare model nel context di nuclio (non su project che è il context nostro)\n",
    "    context.setattr(\"model\", mm)\n",
    "    #setattr(context, \"model\", mm)\n",
    "\n",
    "def serve(context, event):\n",
    "\n",
    "    # Sostanzialmente invochiamo la funzione con una chiamata REST\n",
    "    # Nel body della richiesta mandi l'inference input\n",
    "    \n",
    "    if isinstance(event.body, bytes):\n",
    "        body = json.loads(event.body)\n",
    "    context.logger.info(f\"Received event: {body}\")\n",
    "    inference_input = body[\"inference_input\"]\n",
    "    \n",
    "    return context.model.predict(n=output_chunk_length*2,\n",
    "                                 series=inference_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_NAME=\"serve_model\"\n",
    "func = ml_proj.new_function(name=FUNCTION_NAME,\n",
    "                            kind=\"python\",\n",
    "                            #requirements =[\"darts==0.25.0\", \"pandas==1.4.4\", \"numpy==1.22.4\", \"patsy==0.5.2\", \"scikit-learn==1.1.2\"], # darts was heavy in GB so install torch with no cuda in the run build\n",
    "                            python_version=\"PYTHON3_9\",\n",
    "                            base_image = \"python:3.9\",\n",
    "                            source={\n",
    "                                 \"source\": \"serve_multimodel.py\",\n",
    "                                 \"handler\": \"serve\",\n",
    "                                 \"init_function\": \"init\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_build_model_serve = func.run(action=\"build\",instructions=[\"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\",\"pip3 install darts==0.25.0 pandas==1.4.4 numpy==1.22.4 patsy==0.5.2 scikit-learn==1.1.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds: 0, status: READY ...\n",
      "Seconds: 10, status: READY ...\n",
      "Seconds: 20, status: RUNNING ...\n",
      "Seconds: 30, status: RUNNING ...\n",
      "Seconds: 40, status: RUNNING ...\n",
      "Seconds: 50, status: RUNNING ...\n",
      "Seconds: 60, status: RUNNING ...\n",
      "Seconds: 70, status: RUNNING ...\n",
      "Seconds: 80, status: RUNNING ...\n",
      "Seconds: 90, status: RUNNING ...\n",
      "Seconds: 100, status: RUNNING ...\n",
      "Seconds: 110, status: RUNNING ...\n",
      "Seconds: 120, status: RUNNING ...\n",
      "Seconds: 130, status: RUNNING ...\n",
      "Seconds: 140, status: RUNNING ...\n",
      "Seconds: 150, status: RUNNING ...\n",
      "Seconds: 160, status: RUNNING ...\n",
      "Seconds: 170, status: RUNNING ...\n",
      "Seconds: 180, status: RUNNING ...\n",
      "Seconds: 190, status: RUNNING ...\n",
      "Seconds: 200, status: RUNNING ...\n",
      "Seconds: 210, status: RUNNING ...\n",
      "Seconds: 220, status: RUNNING ...\n",
      "Seconds: 230, status: RUNNING ...\n",
      "Seconds: 240, status: RUNNING ...\n",
      "Seconds: 250, status: RUNNING ...\n",
      "Seconds: 260, status: RUNNING ...\n",
      "Seconds: 270, status: RUNNING ...\n",
      "Seconds: 280, status: RUNNING ...\n",
      "Seconds: 290, status: RUNNING ...\n",
      "Seconds: 300, status: RUNNING ...\n",
      "Seconds: 310, status: RUNNING ...\n",
      "Seconds: 320, status: RUNNING ...\n",
      "Seconds: 330, status: RUNNING ...\n",
      "Seconds: 340, status: RUNNING ...\n",
      "Seconds: 350, status: RUNNING ...\n",
      "Seconds: 360, status: RUNNING ...\n",
      "Seconds: 370, status: RUNNING ...\n",
      "Seconds: 380, status: RUNNING ...\n",
      "Seconds: 390, status: RUNNING ...\n",
      "Seconds: 400, status: RUNNING ...\n",
      "Seconds: 410, status: RUNNING ...\n",
      "Seconds: 420, status: RUNNING ...\n",
      "Seconds: 430, status: RUNNING ...\n",
      "Seconds: 440, status: RUNNING ...\n",
      "Seconds: 450, status: RUNNING ...\n",
      "Seconds: 460, status: RUNNING ...\n",
      "Seconds: 470, status: RUNNING ...\n",
      "Seconds: 480, status: RUNNING ...\n",
      "Finished at 490 seconds\n"
     ]
    }
   ],
   "source": [
    "running_funtion_complete(run_build_model_serve,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_serve_model = func.run(action=\"serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'READY'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_serve_model.status.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running_funtion_complete(run_serve_model,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitalhub-core",
   "language": "python",
   "name": "digitalhub-core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
