{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07163ddc-42ce-48f7-b56b-a493910b732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89067f39-04ee-4568-a725-b2e68df6cd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcheggio</th>\n",
       "      <th>data</th>\n",
       "      <th>posti_liberi</th>\n",
       "      <th>posti_occupati</th>\n",
       "      <th>posti_totali</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VIII Agosto</td>\n",
       "      <td>2024-06-07T01:59:00+00:00</td>\n",
       "      <td>484.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>625</td>\n",
       "      <td>44.500297</td>\n",
       "      <td>11.345368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riva Reno</td>\n",
       "      <td>2024-06-07T02:09:00+00:00</td>\n",
       "      <td>369.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>470</td>\n",
       "      <td>44.501153</td>\n",
       "      <td>11.336062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Riva Reno</td>\n",
       "      <td>2024-06-07T02:19:00+00:00</td>\n",
       "      <td>369.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>470</td>\n",
       "      <td>44.501153</td>\n",
       "      <td>11.336062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VIII Agosto</td>\n",
       "      <td>2024-06-07T02:29:00+00:00</td>\n",
       "      <td>487.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>625</td>\n",
       "      <td>44.500297</td>\n",
       "      <td>11.345368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Riva Reno</td>\n",
       "      <td>2024-06-07T02:29:00+00:00</td>\n",
       "      <td>369.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>470</td>\n",
       "      <td>44.501153</td>\n",
       "      <td>11.336062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44394</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>2024-09-26T08:59:00+00:00</td>\n",
       "      <td>179.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>265</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44395</th>\n",
       "      <td>VIII Agosto</td>\n",
       "      <td>2024-09-26T09:09:00+00:00</td>\n",
       "      <td>374.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>625</td>\n",
       "      <td>44.500297</td>\n",
       "      <td>11.345368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44396</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>2024-09-26T09:09:00+00:00</td>\n",
       "      <td>173.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>265</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44397</th>\n",
       "      <td>Riva Reno</td>\n",
       "      <td>2024-09-26T09:19:00+00:00</td>\n",
       "      <td>331.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>470</td>\n",
       "      <td>44.501153</td>\n",
       "      <td>11.336062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44398</th>\n",
       "      <td>Autostazione</td>\n",
       "      <td>2024-09-26T09:19:00+00:00</td>\n",
       "      <td>171.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>265</td>\n",
       "      <td>44.504422</td>\n",
       "      <td>11.346514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44399 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         parcheggio                       data  posti_liberi  posti_occupati  \\\n",
       "0       VIII Agosto  2024-06-07T01:59:00+00:00         484.0           141.0   \n",
       "1         Riva Reno  2024-06-07T02:09:00+00:00         369.0           101.0   \n",
       "2         Riva Reno  2024-06-07T02:19:00+00:00         369.0           101.0   \n",
       "3       VIII Agosto  2024-06-07T02:29:00+00:00         487.0           138.0   \n",
       "4         Riva Reno  2024-06-07T02:29:00+00:00         369.0           101.0   \n",
       "...             ...                        ...           ...             ...   \n",
       "44394  Autostazione  2024-09-26T08:59:00+00:00         179.0            86.0   \n",
       "44395   VIII Agosto  2024-09-26T09:09:00+00:00         374.0           251.0   \n",
       "44396  Autostazione  2024-09-26T09:09:00+00:00         173.0            92.0   \n",
       "44397     Riva Reno  2024-09-26T09:19:00+00:00         331.0           139.0   \n",
       "44398  Autostazione  2024-09-26T09:19:00+00:00         171.0            94.0   \n",
       "\n",
       "       posti_totali        lat        lon  \n",
       "0               625  44.500297  11.345368  \n",
       "1               470  44.501153  11.336062  \n",
       "2               470  44.501153  11.336062  \n",
       "3               625  44.500297  11.345368  \n",
       "4               470  44.501153  11.336062  \n",
       "...             ...        ...        ...  \n",
       "44394           265  44.504422  11.346514  \n",
       "44395           625  44.500297  11.345368  \n",
       "44396           265  44.504422  11.346514  \n",
       "44397           470  44.501153  11.336062  \n",
       "44398           265  44.504422  11.346514  \n",
       "\n",
       "[44399 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/exports/csv?lang=it&timezone=UTC&use_labels=true&delimiter=%3B\"\n",
    "df = pd.read_csv(URL, sep=\";\")\n",
    "df[['lat', 'lon']] = df['coordinate'].str.split(', ',expand=True)\n",
    "df = df.drop(columns=['% occupazione', 'GUID', 'coordinate']).rename(columns={'Parcheggio': 'parcheggio', 'Data': 'data', 'Posti liberi': 'posti_liberi', 'Posti occupati': 'posti_occupati', 'Posti totali': 'posti_totali'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c00062-dc9f-47a3-9241-899273109de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created project parcheggi-nk-schedulerdigitalhubdev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'parcheggi-nk-schedulerdigitalhubdev'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import digitalhub as dh\n",
    "import getpass as gt\n",
    "\n",
    "PROJECT_NAME = \"parcheggi-nk-scheduler\"+gt.getuser()\n",
    "proj = dh.get_or_create_project(PROJECT_NAME)\n",
    "print(\"created project {}\".format(PROJECT_NAME))\n",
    "PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "295dd1fe-787f-4105-ba4c-2f8e59ee1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e6305e-97ae-4014-801b-b7bc6791546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/download_all.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/download_all.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "import pandas as pd\n",
    "\n",
    "@handler(outputs=[\"dataset\"])\n",
    "def downloader(project, url):\n",
    "    df = url.as_df(file_format='csv',sep=\";\")\n",
    "    df[['lat', 'lon']] = df['coordinate'].str.split(', ',expand=True)\n",
    "    df = df.drop(columns=['% occupazione', 'GUID', 'coordinate']).rename(columns={'Parcheggio': 'parcheggio', 'Data': 'data', 'Posti liberi': 'posti_liberi', 'Posti occupati': 'posti_occupati', 'Posti totali': 'posti_totali'})\n",
    "    df[\"lat\"] = pd.to_numeric(df[\"lat\"])\n",
    "    df[\"lon\"] = pd.to_numeric(df[\"lon\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10be4e0c-b530-4939-924d-0c363234c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = proj.new_function(name=\"downloader-funct\",\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/download_all.py\", \"handler\": \"downloader\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e67fd806-e116-467a-a774-d4f79eb06b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69f14ce5-8f12-4ae3-928b-667845d9691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download = func.run(action=\"job\",inputs={\"url\":di.key},outputs={\"dataset\":\"dataset\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb1bf0-5193-49a1-b6d4-ff39d2c59dcc",
   "metadata": {},
   "source": [
    "# Predict day (Regression SARIMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cdc715b-82b4-442f-a95a-adbea87b0412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/predict_parkings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/predict_parkings.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define a custom function to serialize datetime objects \n",
    "def serialize_datetime(obj): \n",
    "    if isinstance(obj, datetime.datetime): \n",
    "        return obj.isoformat() \n",
    "    raise TypeError(\"Type not serializable\") \n",
    "\n",
    "    \n",
    "def to_point(point):\n",
    "    \"\"\"\n",
    "    Convert a decimal number representing minutes to a datetime object.\n",
    "\n",
    "    Args:\n",
    "        point (float): The decimal number representing minutes.\n",
    "\n",
    "    Returns:\n",
    "        datetime.datetime: A datetime object representing the current date and time with the minutes derived from the input.\n",
    "\n",
    "    Example:\n",
    "        >>> to_point(45)\n",
    "        datetime.datetime(2022, 1, 1, 0, 22, 30)\n",
    "    \"\"\"\n",
    "    today = datetime.datetime.today()\n",
    "    #return datetime.datetime(today.year, today.month, today.day, int(point * 30 / 60), int(point * 30 % 60))\n",
    "    dt = datetime.datetime(today.year, today.month, today.day, int(point * 30 / 60), int(point * 30 % 60))\n",
    "    return json.dumps(dt, default=serialize_datetime)\n",
    "\n",
    "\n",
    "@handler(outputs=[\"parking_data_predicted_regression\"])\n",
    "def predict_day(project, parkings_di):\n",
    "    \"\"\"\n",
    "    Predicts the occupancy of parking spaces for the next 48 steps and saves the results in a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        parkings_di: The data item containing the parking data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert data item to pandas DataFrame\n",
    "    df = parkings_di.as_df()\n",
    "\n",
    "    # Create a clean copy of the DataFrame\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    df_clean = df_clean.drop(columns=['lat', 'lon'])\n",
    "\n",
    "    # Convert 'data' column to datetime\n",
    "    df_clean.data = df_clean.data.astype('datetime64[ms, UTC]')\n",
    "\n",
    "    # Calculate the occupancy rate\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "\n",
    "    # Round the 'data' column to the nearest 30 minutes\n",
    "    df_clean['date_time_slice'] = df_clean.data.dt.round('30min').dt.tz_convert(None)\n",
    "\n",
    "    # Extract the date from the 'data' column\n",
    "    df_clean['date'] = df_clean.data.dt.tz_convert(None)\n",
    "    # df_clean['date'] = df_clean['date'].tz_convert(None)\n",
    "\n",
    "    # Filter out data from the last 30 days\n",
    "    df_clean = df_clean[df_clean.date_time_slice >= (datetime.datetime.today() - pd.DateOffset(30))]\n",
    "\n",
    "    # Filter out data from today\n",
    "    df_clean = df_clean[df_clean.date <= (datetime.datetime.today() - pd.DateOffset(1))]\n",
    "\n",
    "    # Remove the 'date' column\n",
    "    df_clean = df_clean.drop(['date'], axis=1)\n",
    "\n",
    "    # Ensure that 'posti_occupati' is within the range of [0, posti_totali]\n",
    "    df_clean.posti_occupati = df_clean.apply(lambda x: max(0, min(x['posti_totali'], x['posti_occupati'])), axis=1)\n",
    "\n",
    "    # Recalculate the occupancy rate\n",
    "    df_clean['occupied'] = df_clean.posti_occupati / df_clean.posti_totali\n",
    "\n",
    "    # Get unique parking locations\n",
    "    parcheggi = df_clean['parcheggio'].unique()\n",
    "\n",
    "    # Initialize a list to store the predictions\n",
    "    res = []\n",
    "\n",
    "    # Iterate over each parking location\n",
    "    for parcheggio in parcheggi:\n",
    "        # Create a copy of the cleaned DataFrame\n",
    "        cp = df_clean.copy()\n",
    "\n",
    "        # Filter data for the current parking location\n",
    "        parc_df = cp[cp['parcheggio'] == parcheggio]\n",
    "\n",
    "        # Group data by 'date_time_slice' and aggregate metrics\n",
    "        parc_df = parc_df.groupby('date_time_slice').agg({'posti_occupati':['sum','count'], 'posti_totali':['sum','count']})\n",
    "\n",
    "        # Calculate the occupancy rate\n",
    "        parc_df['occupied'] = parc_df.posti_occupati['sum'] / parc_df.posti_totali['sum']\n",
    "\n",
    "        # Remove unnecessary columns\n",
    "        parc_df.drop(columns=['posti_occupati', 'posti_totali'], inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by index\n",
    "        parc_df.sort_index(inplace=True)\n",
    "\n",
    "        # Extract the 'occupied' column as a Series\n",
    "        data = parc_df.reset_index()['occupied']\n",
    "\n",
    "        # Define the SARIMA model parameters\n",
    "        my_seasonal_order = (1, 1, 1, 48)\n",
    "\n",
    "        # Create and fit the SARIMA model\n",
    "        sarima_model = SARIMAX(data, order=(1, 0, 1), seasonal_order=my_seasonal_order)\n",
    "        results_SAR = sarima_model.fit(disp=-1)\n",
    "\n",
    "        # Generate predictions for the next 48 steps\n",
    "        pred = results_SAR.forecast(steps=48).reset_index()\n",
    "\n",
    "        # Add the 'parcheggio' column\n",
    "        pred['parcheggio'] = parcheggio\n",
    "        res.append(pred)\n",
    "    \n",
    "    for pred in res:\n",
    "        pred['point'] = (pred.index).astype('int')\n",
    "        pred['datetime'] = pred['point'].apply(to_point)\n",
    "        pred.drop(['point'], axis=1, inplace=True)\n",
    "    \n",
    "    all = pd.concat(res, ignore_index=True)[['predicted_mean', 'parcheggio', 'datetime']]\n",
    "\n",
    "    USERNAME = os.getenv(\"POSTGRES_USER\")\n",
    "    PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "    engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/digitalhub')\n",
    "    with engine.connect() as connection: \n",
    "        try: connection.execute(\"DELETE FROM parkings_prediction\")\n",
    "        except: pass\n",
    "\n",
    "    all.to_sql('parkings_prediction', engine, if_exists=\"append\")\n",
    "\n",
    "    # old_pd = all\n",
    "    # try: \n",
    "    #     dat_old = project.get_dataitem('parking_prediction_sarima_model')\n",
    "    #     old_pd = pd.concat([dat_old.as_df(), all], ignore_index=True)\n",
    "    # except: pass\n",
    "    # project.log_dataitem(name='parking_prediction_sarima_model', data=old_pd, kind=\"table\")\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0167c5e-f105-46ac-b61f-05062a990ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = proj.new_function(name=\"predict-day\",\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_9\",\n",
    "                         source={\"source\": \"src/predict_parkings.py\", \"handler\": \"predict_day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ccd395e-9d70-47bd-8391-93b8a6e4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download = proj.get_dataitem(\"dataset\").key\n",
    "run_parkings = func.run(action=\"job\",inputs={\"parkings_di\":data_item_download},outputs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66add694-7e6a-42a3-8510-65c78256d804",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc271c-16a1-462e-ba6d-774518c1f7fd",
   "metadata": {},
   "source": [
    "In this step we will create a workflow pipeline whose purpose is to call the download and predict-day(regression) function based on schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a09968f-bf89-4077-9bf1-37785054e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/parkings_pipeline_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/parkings_pipeline_regression.py\"\n",
    "\n",
    "from digitalhub_runtime_kfp.dsl import pipeline_context\n",
    "\n",
    "def myhandler(url):\n",
    "    with pipeline_context() as pc:\n",
    "        s1_dataset = pc.step(name=\"download\", function=\"downloader-funct\", action=\"job\", inputs={\"url\":url},outputs={\"dataset\":\"dataset\"})\n",
    "        s2_predict = pc.step(name=\"predict\", function=\"predict-day\", action=\"job\", inputs={\"parkings_di\":s1_dataset.outputs['dataset']}, outputs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00957ebb-4ac5-4aed-a945-0969cde9bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = proj.new_workflow(name=\"pipeline_parcheggi_regression\", kind=\"kfp\", source={\"source\": \"src/parkings_pipeline_regression.py\", \"handler\": \"myhandler\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5724c9a-24a8-4dbc-9967-69ec552ad099",
   "metadata": {},
   "outputs": [],
   "source": [
    "di= proj.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n",
    "c = workflow.run(parameters={\"url\": di.key})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57888a2b-f8fa-4195-a2c0-8295a9d9702b",
   "metadata": {},
   "source": [
    "## Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d4e117-d111-433a-84f6-5fa6e66b1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run(parameters={\"url\": di.key}, schedule=\"@hourly\")\n",
    "#workflow.run(parameters={\"url\": di.key}, schedule=\"*/5 * * * *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9411e-5ac9-4c6f-b1e1-32a100d9b57f",
   "metadata": {},
   "source": [
    "# Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fdd08-6ab5-4623-a114-217d2e98ee60",
   "metadata": {},
   "source": [
    "In this step we will create Monitor workflow pipeline based on schedule, whose purpose is to call\n",
    "\n",
    "1) call the service created after training a data prediction model using darts framework and NBEATS Deep Learning model. (see notebook parcheggi_ml.ipynb)\n",
    "2) save the prediction in database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a99938-986b-4a73-85c5-ff76fca6accf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
