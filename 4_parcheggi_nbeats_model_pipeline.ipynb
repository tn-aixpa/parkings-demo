{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a563fb50-1483-4eb3-a140-8e7ad24c2a85",
   "metadata": {},
   "source": [
    "# Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0cc1de-0172-4def-9bf2-40f25c3bfe25",
   "metadata": {},
   "source": [
    "In this step we will create Monitor workflow pipeline based on schedule, whose purpose is to call\n",
    "\n",
    "1) call the service created after training a data prediction model using darts framework and NBEATS Deep Learning model. (see notebook parcheggi_ml.ipynb)\n",
    "2) save the prediction in database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96edd9-21a1-4460-9769-0c98ce27b7fa",
   "metadata": {},
   "source": [
    "## Platform Support - Data Ops\n",
    "We use the platform support to read the data created into the platform after the execution of notebook(parcheggi_data_pipeline.ipynb) for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea73c54-438f-4aab-8f2b-a0f6efd5e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import digitalhub as dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a631269-98e6-481d-ae39-06aebc668617",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/exports/csv?lang=it&timezone=UTC&use_labels=true&delimiter=%3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1aee01-269c-49ca-a6b1-a2234d88861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"parcheggi-scheduler\"\n",
    "proj = dh.get_or_create_project(PROJECT_NAME)\n",
    "print(\"created project {}\".format(PROJECT_NAME))\n",
    "PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f802ae-ef0b-4e9a-b605-c069f9dc110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download = proj.get_dataitem(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35b01b-b15d-47d1-bd78-1c650c78f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkings_df = data_item_download.as_df()\n",
    "# parkings_df[parkings_df.columns[0]].count()\n",
    "parkings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fc41a-59b4-4430-81fe-48b693495d60",
   "metadata": {},
   "source": [
    "In this script, one needs to update the 'serve' RUN id of the NBEATSModel service. From the project console, go to RUNS(model_serve) in RUNNING state, and copy the identifier value (last part of key value) \n",
    "\n",
    "**project.get_run(identifier='f4823893-1785-4a14-aeb3-99335b64f0fb')**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e515b-347d-4ab0-9a9f-5c3b509087b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/predict_nbeats_timeseries.py\"\n",
    "from digitalhub_runtime_python import handler\n",
    "from sqlalchemy import create_engine\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import digitalhub as dh\n",
    "\n",
    "@handler()\n",
    "def predict_day(project,  parkings_di):\n",
    "    \"\"\"\n",
    "    Monitor and predict parking occupancy.\n",
    "    \"\"\"\n",
    "\n",
    "    # get serving predictor function run\n",
    "    run_serve_model =  project.get_run(identifier='3067478c-5da4-4190-a587-73a1ddb3fac9')\n",
    "    \n",
    "    # get current date and time as string\n",
    "    date_str = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    # get parkings dataset and convert it to a dataframe\n",
    "    parkings_df = parkings_di.as_df()\n",
    "\n",
    "    # initialize an empty dataframe for predictions\n",
    "    pred_df = pd.DataFrame(columns=['parcheggio', 'datetime', 'predicted_mean'])\n",
    "\n",
    "    # iterate over each parking in the dataset\n",
    "    parcheggi =  parkings_df['parcheggio'].unique()\n",
    "    #parcheggi = ['Riva Reno' ,'VIII Agosto']\n",
    "    for parking_str in parcheggi:\n",
    "        # construct API URL based on parking and current date\n",
    "        API_URL = f'https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/records?where=data%3C%3D%27{date_str}%27%20and%20parcheggio%3D%27{parking_str}%27&order_by=data%20DESC&limit=100'\n",
    "\n",
    "        # define the file to store the latest data\n",
    "        latest_data_file = 'last_records.json'\n",
    "\n",
    "        # fetch data from the API and save it to a file\n",
    "        with requests.get(API_URL) as r:\n",
    "            with open(latest_data_file, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "        # read the latest data from the file and process it\n",
    "        with open(latest_data_file) as f:\n",
    "            json_data = json.load(f)\n",
    "            df_latest = pd.json_normalize(json_data['results']).drop(columns=['guid', 'occupazione']).rename(columns={\"coordinate.lon\": \"lon\", \"coordinate.lat\": \"lat\"})\n",
    "            df_latest.data = df_latest.data.astype('datetime64[ns, UTC]')\n",
    "            df_latest['value'] = df_latest.posti_occupati / df_latest.posti_totali\n",
    "            df_latest['date'] = df_latest.data.dt.round('30min')\n",
    "            df_latest = df_latest.drop(columns=['parcheggio'])\n",
    "            df_latest = df_latest.groupby('date').agg({'value': 'mean'})\n",
    "\n",
    "        # convert the processed data to JSON and make a request to the serving predictor function\n",
    "        jsonstr = df_latest.reset_index().to_json(orient='records')\n",
    "        arr = json.loads(jsonstr)\n",
    "        SERVICE_URL = run_serve_model.status.to_dict()[\"service\"][\"url\"]\n",
    "        with requests.post(f'http://{SERVICE_URL}', json={\"inference_input\":arr}) as r:\n",
    "            res = json.loads(r.content)\n",
    "        res_df = pd.DataFrame(res)\n",
    "        res_df['datetime'] = res_df['date']\n",
    "        res_df['parcheggio'] = parking_str\n",
    "        res_df['predicted_mean'] = res_df['value']\n",
    "        res_df = res_df.drop(columns=['date', 'value'])\n",
    "        pred_df = pd.concat([pred_df, res_df], ignore_index=True)\n",
    "        \n",
    "        \n",
    "    # write data to database\n",
    "    USERNAME = os.getenv(\"POSTGRES_USER\")\n",
    "    PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "    engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/digitalhub')\n",
    "    # save in db.\n",
    "    with engine.connect() as connection:\n",
    "        try: connection.execute(\"DELETE FROM parkings_prediction_nbeats WHERE datetime < now() - interval '30 days'\")\n",
    "        except: pass\n",
    "    with engine.connect() as connection:\n",
    "        try: connection.execute(\"DELETE FROM parkings_prediction_nbeats_sliced WHERE datetime < now() - interval '30 days'\")\n",
    "        except: pass\n",
    "                \n",
    "    pred_df['datetime'] = pd.to_datetime(pred_df['datetime'], unit='ms')\n",
    "    # read existing table\n",
    "    query = 'select * from parkings_prediction_nbeats'\n",
    "    saved_df = pd.DataFrame()\n",
    "    try:\n",
    "        saved_df = pd.read_sql_query(query, engine)\n",
    "        saved_df = saved_df.drop(columns=['index'])\n",
    "    except: pass\n",
    "        \n",
    "    new_df = pd.concat([saved_df, pred_df])\n",
    "    new_df = new_df.drop_duplicates(subset=['parcheggio', 'datetime'])\n",
    "    new_df.to_sql('parkings_prediction_nbeats', engine, if_exists=\"replace\")\n",
    "    new_df['slice_datetime'] = datetime.datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S')\n",
    "    new_df.to_sql('parkings_prediction_nbeats_sliced', engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf3dae-a96e-4816-99da-f04dd69f60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = proj.new_function(name=\"predict-day-nbeats-model\",\n",
    "                         kind=\"python\",\n",
    "                         python_version=\"PYTHON3_10\",\n",
    "                         source={\"source\": \"src/predict_nbeats_timeseries.py\", \"handler\": \"predict_day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013317b-cc27-4e3f-b0fd-62007689082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_download = proj.get_dataitem(\"dataset\").key\n",
    "run_monitor_parkings = func.run(action=\"job\",inputs={\"parkings_di\": data_item_download},outputs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19910b70-1ebe-4681-a16b-c34db5a0f741",
   "metadata": {},
   "source": [
    "Wait until prediction run is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d6cd3-4cac-4042-bafa-6fe1a9f287e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_monitor_parkings.refresh().status.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e2ab6-9d43-4b28-bd7c-cffff08d569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from digitalhub_runtime_python import handler\n",
    "# from sqlalchemy import create_engine\n",
    "# import datetime \n",
    "# import requests\n",
    "# import json\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import digitalhub as dh\n",
    "\n",
    "# # get serving predictor function run\n",
    "# run_serve_model =  proj.get_run(identifier='3067478c-5da4-4190-a587-73a1ddb3fac9')\n",
    "\n",
    "# # get current date and time as string\n",
    "# date_str = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# # get parkings dataset and convert it to a dataframe\n",
    "# parkings_df = data_item_download.as_df()\n",
    "\n",
    "# # initialize an empty dataframe for predictions\n",
    "# pred_df = pd.DataFrame(columns=['parcheggio', 'datetime', 'predicted_mean'])\n",
    "\n",
    "# # iterate over each parking in the dataset\n",
    "# parcheggi =  parkings_df['parcheggio'].unique()\n",
    "# for parking_str in parcheggi:\n",
    "#     # construct API URL based on parking and current date\n",
    "#     API_URL = f'https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/disponibilita-parcheggi-storico/records?where=data%3C%3D%27{date_str}%27%20and%20parcheggio%3D%27{parking_str}%27&order_by=data%20DESC&limit=100'\n",
    "\n",
    "#     # define the file to store the latest data\n",
    "#     latest_data_file = 'last_records.json'\n",
    "\n",
    "#     # fetch data from the API and save it to a file\n",
    "#     with requests.get(API_URL) as r:\n",
    "#         with open(latest_data_file, \"wb\") as f:\n",
    "#             f.write(r.content)\n",
    "\n",
    "#     # read the latest data from the file and process it\n",
    "#     with open(latest_data_file) as f:\n",
    "#         json_data = json.load(f)\n",
    "#         df_latest = pd.json_normalize(json_data['results']).drop(columns=['guid', 'occupazione']).rename(columns={\"coordinate.lon\": \"lon\", \"coordinate.lat\": \"lat\"})\n",
    "#         df_latest.data = df_latest.data.astype('datetime64[ns, UTC]')\n",
    "#         df_latest['value'] = df_latest.posti_occupati / df_latest.posti_totali\n",
    "#         df_latest['date'] = df_latest.data.dt.round('30min')\n",
    "#         df_latest = df_latest.drop(columns=['parcheggio'])\n",
    "#         df_latest = df_latest.groupby('date').agg({'value': 'mean'})\n",
    "\n",
    "#     # convert the processed data to JSON and make a request to the serving predictor function\n",
    "#     jsonstr = df_latest.reset_index().to_json(orient='records')\n",
    "#     arr = json.loads(jsonstr)\n",
    "#     SERVICE_URL = run_serve_model.status.to_dict()[\"service\"][\"url\"]\n",
    "#     with requests.post(f'http://{SERVICE_URL}', json={\"inference_input\":arr}) as r:\n",
    "#         res = json.loads(r.content)\n",
    "#     res_df = pd.DataFrame(res)\n",
    "#     res_df['datetime'] = res_df['date']\n",
    "#     res_df['parcheggio'] = parking_str\n",
    "#     res_df['predicted_mean'] = res_df['value']\n",
    "#     res_df = res_df.drop(columns=['date', 'value'])\n",
    "#     pred_df = pd.concat([pred_df, res_df], ignore_index=True)\n",
    "\n",
    "#     # write data to database\n",
    "#     USERNAME = os.getenv(\"POSTGRES_USER\")\n",
    "#     PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "#     engine = create_engine('postgresql+psycopg2://'+USERNAME+':'+PASSWORD+'@database-postgres-cluster/digitalhub')\n",
    "    \n",
    "# # save in db.\n",
    "# with engine.connect() as connection:\n",
    "#     try: connection.execute(\"DELETE FROM parkings_prediction_nbeats WHERE datetime < now() - interval '30 days'\")\n",
    "#     except: pass\n",
    "\n",
    "# # save in db.\n",
    "# with engine.connect() as connection:\n",
    "#     try: connection.execute(\"DELETE FROM parkings_prediction_nbeats_sliced WHERE datetime < now() - interval '30 days'\")\n",
    "#     except: pass\n",
    "\n",
    "\n",
    "# pred_df['datetime'] = pd.to_datetime(pred_df['datetime'], unit='ms')\n",
    "\n",
    "# query = 'select * from parkings_prediction_nbeats'\n",
    "# saved_df = pd.DataFrame()\n",
    "# try:\n",
    "#     saved_df = pd.read_sql_query(query, engine)\n",
    "#     saved_df = saved_df.drop(columns=['index'])\n",
    "# except: pass\n",
    "    \n",
    "# new_df = pd.concat([saved_df, pred_df])\n",
    "# new_df = new_df.drop_duplicates(subset=['parcheggio', 'datetime'])\n",
    "# len(new_df)\n",
    "# new_df.to_sql('parkings_prediction_nbeats', engine, if_exists=\"replace\")\n",
    "# new_df['slice_datetime'] = date_str\n",
    "# new_df.to_sql('parkings_prediction_nbeats_sliced', engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54451d-e5d4-4f33-bb9a-0e10b7bdd171",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038cdb6-790c-4b9c-85d0-c16bce600dd9",
   "metadata": {},
   "source": [
    "In this step we will create a workflow pipeline whose purpose is to call the download function to fetch data and pass it to predict_day function which produce prediction based on NBEATS model. The entire workflow is scheduled for frequent runs based on frequrency provided using CRON expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b12490-b25b-4c76-8d5e-6b5ceaf28289",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"src/parkings_pipeline_nbeats_model.py\"\n",
    "\n",
    "from digitalhub_runtime_kfp.dsl import pipeline_context\n",
    "\n",
    "def myhandler(di):\n",
    "    with pipeline_context() as pc:\n",
    "        s2_predict = pc.step(name=\"predict-day-nbeats-model\", function=\"predict-day-nbeats-model\", action=\"job\", inputs={\"parkings_di\":di}, outputs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478efdc-6051-4e2e-abe1-691781f6fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = proj.new_workflow(name=\"pipeline_parcheggi_nbeats_model\", kind=\"kfp\", source={\"source\": \"src/parkings_pipeline_nbeats_model.py\", \"handler\": \"myhandler\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695936ab-2f87-46c4-8b56-3e1505abe428",
   "metadata": {},
   "source": [
    "## Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf15f97-d989-45de-bb13-e6988a685045",
   "metadata": {},
   "source": [
    "Nbeats model Pipeline workflow is scheduled for frequent runs using Crons expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5d731-4eff-4750-b6c9-883c017e8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = proj.get_dataitem(\"dataset\").key\n",
    "# workflow.run(parameters={\"di\": di})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009d80b-0180-4807-9261-09b4e40f5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run(parameters={\"di\": di}, schedule=\"0 8-18 * * *\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
